{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTAEvNNFGvOE"
   },
   "source": [
    "Notes:\n",
    "\n",
    "Please change the path to the data files. We wrote code to load the files for each section for convenience.\n",
    "\n",
    "Each big section is self-contained and there are comments in each portion of code to explain the flow. Please run it linearly in each section for the code to work smoothly.\n",
    "\n",
    "We ran our code on colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bg15Z6QyY9c4"
   },
   "source": [
    "# Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3y4wHYIdY0w3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from google.colab import drive\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "#drive.uount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stvm7jQNgmux"
   },
   "source": [
    "# **CNN Based Architectures for Motor Imagery Classification**\n",
    "\n",
    "\n",
    "We develop several CNN architectures in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiCyNg7HZtBW"
   },
   "source": [
    "### **Data Loading** \n",
    "\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1jziXejZQM1"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.load(\"path_name")\n",
    "y_test = np.load(\"path_name")\n",
    "person_train_valid = np.load(\"path_name")\n",
    "X_train_valid = np.load(\"path_name")\n",
    "y_train_valid = np.load(\"path_name")\n",
    "person_test = np.load(\"path_name")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p865YZVcZQM4"
   },
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "oTd1vza7ZQM4",
    "outputId": "39de3d92-fecc-4cce-9f9d-471a8b92a26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 25, 1000)\n",
      "Test data shape: (443, 25, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "og_cjRhKcs-l"
   },
   "source": [
    "## **Random Splitting and shaping of the Data**\n",
    "\n",
    "\n",
    "Splitting of the X_train_valid_data. We keep 500 data points out of 2115 for validation.\n",
    "\n",
    "We also reshape the data into a 4 dimensional tensor for inputting into CNN later.\n",
    "\n",
    "We also make the labels one hot encoded for inputting into cross entropy function for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "7rzoL8uWZQM-",
    "outputId": "8d5b94c1-c3dc-4651-99c9-8a351aba8bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1615, 1000, 1, 25)\n",
      "(1615, 4)\n",
      "(443, 1000, 1, 25)\n",
      "(443, 4)\n",
      "(500, 1000, 1, 25)\n",
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "# random splitting\n",
    "ind_valid = np.random.choice(2115, 500, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "(x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 4)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "\n",
    "x_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_test.shape)\n",
    "print (x_valid.shape)\n",
    "print (y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZiRyRHOJHrI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxTZeZUCXFYr"
   },
   "source": [
    "## **Shallow CNN (SCNN)**\n",
    "\n",
    "** Layer 1**:  25 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ Flatten\n",
    "\n",
    "**Layer 2:** FCNet with 4 outputs.\n",
    "\n",
    "We train the SCNN with optimal hyperparameters (learning date) in the first block below. In the second block, we fix the learned hyperparameter and run the model multiple times to find mean test accuracy and standard deviation of test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "MML5rFqaXSOX",
    "outputId": "e140c033-d02f-4ed6-a543-836bdc91d124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Optimal learning rate: 0.001\n",
      "\n",
      " Validation accuracy overall w/o dropout bn: 0.542\n",
      "\n",
      " Test accuracy overall: 0.5395034\n",
      "Test accuracy person 0 0.36\n",
      "Test accuracy person 1 0.62\n",
      "Test accuracy person 2 0.4\n",
      "Test accuracy person 3 0.5\n",
      "Test accuracy person 4 0.61702126\n",
      "Test accuracy person 5 0.48979592\n",
      "Test accuracy person 6 0.78\n",
      "Test accuracy person 7 0.66\n",
      "Test accuracy person 8 0.42553192\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = [1e-4, 5e-4, 1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=50,\n",
    "             validation_data=(x_valid, y_valid), verbose=False)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters =  learning_rate\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "print (\"Optimal learning rate: \" +str(parameters) )\n",
    "\n",
    "score = model_max.evaluate(x_valid, y_valid, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Validation accuracy overall w/o dropout bn:', score[1]) \n",
    "score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "for i in range(9):    \n",
    "  score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "  print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-UzeaGmYHSt"
   },
   "source": [
    "### **Shallow CNN (SCNN): Multiple Runs**\n",
    "\n",
    "We run SCNN with optimal learning rate selected in the previous block multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "B22lC23J3i1d",
    "outputId": "f512e77c-e630-4a9b-9eb2-b9ed33d96a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy overall w/o bn w/o dropout: 0.47539999783039094 with sd 0.019719020603701856\n",
      "Validation accuracy persons w/o bn w/o dropout: [0.43953489 0.41698113 0.33793103 0.41607143 0.56268657 0.41403509\n",
      " 0.685      0.5111111  0.45769231] with sd [0.04816817 0.0536659  0.04765612 0.04377732 0.03951702 0.08637568\n",
      " 0.04179979 0.03989011 0.06007884]\n",
      "Test accuracy overall w/o bn w/o dropout: 0.490067720413208 with sd 0.021449451255516012\n",
      "Test accuracy persons w/o bn w/o dropout: [0.386      0.496      0.372      0.492      0.66382977 0.39591837\n",
      " 0.698      0.492      0.41914894] with sd [0.06390618 0.0591946  0.05075431 0.056      0.04125684 0.06270323\n",
      " 0.06225753 0.05741079 0.08406275]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate_list = [1e-4]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "N_iter             = 10\n",
    "score_arr          = np.zeros(N_iter)\n",
    "score_person       = np.zeros((9,N_iter))\n",
    "score_arr_valid          = np.zeros(N_iter)\n",
    "score_person_valid       = np.zeros((9,N_iter))\n",
    "for n in range(N_iter):\n",
    "    for learning_rate in learning_rate_list:\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "      model.add(tf.keras.layers.Flatten())\n",
    "      model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "      optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "      model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=['accuracy'])\n",
    "  #     model.summary()\n",
    "      model.fit(x_train,\n",
    "               y_train,\n",
    "               batch_size=64,\n",
    "               epochs=50,\n",
    "               validation_data=(x_valid, y_valid), verbose=False)\n",
    "      score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "\n",
    "  score_arr_valid[n] = model.evaluate(x_valid, y_valid, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person_valid[i,n] = model.evaluate(x_valid[person_train_valid[ind_valid].T[0]==i], y_valid[person_train_valid[ind_valid].T[0]==i], verbose=0)[1]        \n",
    "\n",
    "\n",
    "  score_arr[n] = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person[i,n] = model.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)[1]    \n",
    "    \n",
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall w/o bn w/o dropout: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons w/o bn w/o dropout: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n",
    "\n",
    "score_final_overall = np.mean(score_arr)  \n",
    "score_final_std     = np.std(score_arr)\n",
    "score_final_person        = np.mean(score_person, axis=1)\n",
    "score_final_person_std    = np.std(score_person, axis=1)\n",
    "print('Test accuracy overall w/o bn w/o dropout: ' +  str(score_final_overall) + ' with sd ' + str(score_final_std))\n",
    "print('Test accuracy persons w/o bn w/o dropout: ' +  str(score_final_person)+ ' with sd ' + str(score_final_person_std)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ojadwtup3qh9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1pZAQLkDK01H",
    "outputId": "d12e03cf-fdd7-4a1e-f760-34a57e3638c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy overall w/o bn w/o dropout: 0.48080000281333923 with sd 0.009516303800984272\n",
      "Validation accuracy persons w/o bn w/o dropout: [0.47674419 0.44716981 0.36551724 0.4125     0.57910448 0.39649122\n",
      " 0.69166666 0.47962963 0.44423077] with sd [0.06347602 0.04701863 0.05707912 0.04687287 0.04566286 0.06335253\n",
      " 0.06759765 0.05940376 0.0489913 ]\n"
     ]
    }
   ],
   "source": [
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall w/o bn w/o dropout: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons w/o bn w/o dropout: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8v5ghUPMt1d"
   },
   "source": [
    "## **SCNN-dp: Shallow CNN with dropout and w/o bn.**\n",
    "\n",
    "** Layer 1**:  25 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ Dropout  $\\rightarrow$ Flatten\n",
    "\n",
    "**Layer 2:** FCNet with 4 outputs.\n",
    "\n",
    "We train the SCNN-dp with optimal hyperparameters (learning rate, dropout) in the block below. In the second block, we fix the learned hyperparameter and run the model multiple times to find mean test accuracy and standard deviation of test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18309
    },
    "colab_type": "code",
    "id": "8gge-8OqKd7V",
    "outputId": "57413b8d-f47b-4e6a-ecc6-a305197be765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_73 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 8.7693 - acc: 0.2731 - val_loss: 8.3230 - val_acc: 0.2760\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8350 - acc: 0.3220 - val_loss: 7.6768 - val_acc: 0.2720\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.7176 - acc: 0.3659 - val_loss: 7.3578 - val_acc: 0.3100\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.3682 - acc: 0.3858 - val_loss: 6.6824 - val_acc: 0.3240\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.6745 - acc: 0.4223 - val_loss: 6.0134 - val_acc: 0.3540\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.0020 - acc: 0.4706 - val_loss: 5.5317 - val_acc: 0.3800\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.8185 - acc: 0.4737 - val_loss: 5.1974 - val_acc: 0.3920\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1236 - acc: 0.5189 - val_loss: 4.9696 - val_acc: 0.4020\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.7114 - acc: 0.5542 - val_loss: 4.8501 - val_acc: 0.4320\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.3506 - acc: 0.5858 - val_loss: 4.4733 - val_acc: 0.4620\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1269 - acc: 0.6012 - val_loss: 4.4074 - val_acc: 0.4560\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8487 - acc: 0.6080 - val_loss: 4.1800 - val_acc: 0.4620\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.5856 - acc: 0.6446 - val_loss: 4.3841 - val_acc: 0.4700\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.5971 - acc: 0.6433 - val_loss: 4.0145 - val_acc: 0.4820\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.3475 - acc: 0.6669 - val_loss: 3.9038 - val_acc: 0.4900\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1506 - acc: 0.6873 - val_loss: 3.8829 - val_acc: 0.4960\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.0149 - acc: 0.6966 - val_loss: 3.7229 - val_acc: 0.5140\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.8002 - acc: 0.7121 - val_loss: 3.8703 - val_acc: 0.4980\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.7590 - acc: 0.7288 - val_loss: 3.6333 - val_acc: 0.5200\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6336 - acc: 0.7424 - val_loss: 3.6257 - val_acc: 0.5540\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.5195 - acc: 0.7467 - val_loss: 3.5694 - val_acc: 0.5320\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4340 - acc: 0.7591 - val_loss: 3.5976 - val_acc: 0.5260\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2081 - acc: 0.7889 - val_loss: 3.5994 - val_acc: 0.5240\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2522 - acc: 0.7672 - val_loss: 3.6994 - val_acc: 0.5300\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0993 - acc: 0.7944 - val_loss: 3.4769 - val_acc: 0.5400\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1209 - acc: 0.7901 - val_loss: 3.4953 - val_acc: 0.5640\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0267 - acc: 0.8124 - val_loss: 3.4541 - val_acc: 0.5380\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9635 - acc: 0.8130 - val_loss: 3.4265 - val_acc: 0.5460\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.8904 - acc: 0.8297 - val_loss: 3.3202 - val_acc: 0.5560\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.8349 - acc: 0.8365 - val_loss: 3.3239 - val_acc: 0.5520\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9169 - acc: 0.8173 - val_loss: 3.3423 - val_acc: 0.5400\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7604 - acc: 0.8495 - val_loss: 3.2924 - val_acc: 0.5460\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7851 - acc: 0.8446 - val_loss: 3.3083 - val_acc: 0.5360\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6829 - acc: 0.8545 - val_loss: 3.2495 - val_acc: 0.5480\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7166 - acc: 0.8489 - val_loss: 3.1089 - val_acc: 0.5520\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5969 - acc: 0.8700 - val_loss: 3.2291 - val_acc: 0.5560\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6262 - acc: 0.8613 - val_loss: 3.1748 - val_acc: 0.5460\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5756 - acc: 0.8731 - val_loss: 3.1995 - val_acc: 0.5640\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5787 - acc: 0.8625 - val_loss: 3.1478 - val_acc: 0.5700\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4718 - acc: 0.8805 - val_loss: 3.1032 - val_acc: 0.5580\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4513 - acc: 0.8978 - val_loss: 3.0778 - val_acc: 0.5700\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4996 - acc: 0.8805 - val_loss: 3.1438 - val_acc: 0.5680\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4615 - acc: 0.8923 - val_loss: 3.0951 - val_acc: 0.5580\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4356 - acc: 0.8991 - val_loss: 3.0774 - val_acc: 0.5660\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4027 - acc: 0.9022 - val_loss: 3.1024 - val_acc: 0.5500\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3623 - acc: 0.9127 - val_loss: 3.1355 - val_acc: 0.5600\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3843 - acc: 0.9084 - val_loss: 3.0676 - val_acc: 0.5600\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3198 - acc: 0.9176 - val_loss: 3.0585 - val_acc: 0.5660\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3405 - acc: 0.9127 - val_loss: 3.0115 - val_acc: 0.5680\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4076 - acc: 0.9046 - val_loss: 2.9782 - val_acc: 0.5780\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_74 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 10.8116 - acc: 0.2638 - val_loss: 9.6035 - val_acc: 0.2660\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.8536 - acc: 0.3220 - val_loss: 7.9752 - val_acc: 0.3300\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.3878 - acc: 0.4254 - val_loss: 5.2121 - val_acc: 0.4200\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1985 - acc: 0.5344 - val_loss: 4.4935 - val_acc: 0.4680\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7244 - acc: 0.6539 - val_loss: 3.7631 - val_acc: 0.5460\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.7768 - acc: 0.7362 - val_loss: 3.6669 - val_acc: 0.5480\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4069 - acc: 0.7672 - val_loss: 3.4636 - val_acc: 0.5580\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4165 - acc: 0.7641 - val_loss: 4.0496 - val_acc: 0.5360\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1598 - acc: 0.7920 - val_loss: 3.2962 - val_acc: 0.5480\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7369 - acc: 0.8464 - val_loss: 3.1981 - val_acc: 0.5760\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5533 - acc: 0.8873 - val_loss: 3.1680 - val_acc: 0.5680\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4612 - acc: 0.8929 - val_loss: 3.1700 - val_acc: 0.5880\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3894 - acc: 0.9158 - val_loss: 3.1167 - val_acc: 0.5600\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3875 - acc: 0.9146 - val_loss: 2.9343 - val_acc: 0.5840\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3189 - acc: 0.9368 - val_loss: 3.0798 - val_acc: 0.5840\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3068 - acc: 0.9276 - val_loss: 2.9779 - val_acc: 0.6000\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2852 - acc: 0.9300 - val_loss: 3.0012 - val_acc: 0.5820\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2759 - acc: 0.9350 - val_loss: 3.1018 - val_acc: 0.5800\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2346 - acc: 0.9449 - val_loss: 2.9913 - val_acc: 0.5900\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1959 - acc: 0.9523 - val_loss: 3.2984 - val_acc: 0.5660\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2556 - acc: 0.9406 - val_loss: 3.1917 - val_acc: 0.5800\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1979 - acc: 0.9536 - val_loss: 3.3883 - val_acc: 0.5820\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1628 - acc: 0.9591 - val_loss: 3.0913 - val_acc: 0.5780\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1561 - acc: 0.9573 - val_loss: 3.3032 - val_acc: 0.5780\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1486 - acc: 0.9628 - val_loss: 3.2007 - val_acc: 0.5860\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1389 - acc: 0.9647 - val_loss: 3.3820 - val_acc: 0.5880\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1549 - acc: 0.9628 - val_loss: 3.2204 - val_acc: 0.6020\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1579 - acc: 0.9591 - val_loss: 3.2367 - val_acc: 0.5920\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1285 - acc: 0.9721 - val_loss: 3.1539 - val_acc: 0.6060\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0770 - acc: 0.9839 - val_loss: 3.3497 - val_acc: 0.5980\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1049 - acc: 0.9783 - val_loss: 3.2639 - val_acc: 0.6200\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1444 - acc: 0.9635 - val_loss: 3.1019 - val_acc: 0.6100\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0890 - acc: 0.9789 - val_loss: 3.4501 - val_acc: 0.6000\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0990 - acc: 0.9777 - val_loss: 3.5553 - val_acc: 0.6000\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1173 - acc: 0.9740 - val_loss: 3.1779 - val_acc: 0.6040\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0940 - acc: 0.9789 - val_loss: 3.2351 - val_acc: 0.5960\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0702 - acc: 0.9870 - val_loss: 3.1773 - val_acc: 0.6060\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0851 - acc: 0.9827 - val_loss: 3.3585 - val_acc: 0.6060\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0855 - acc: 0.9808 - val_loss: 3.3318 - val_acc: 0.5920\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0874 - acc: 0.9833 - val_loss: 3.2879 - val_acc: 0.6140\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0646 - acc: 0.9889 - val_loss: 3.3176 - val_acc: 0.6140\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0620 - acc: 0.9907 - val_loss: 3.3546 - val_acc: 0.5980\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0808 - acc: 0.9789 - val_loss: 3.1977 - val_acc: 0.6160\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0662 - acc: 0.9876 - val_loss: 3.0119 - val_acc: 0.6260\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0651 - acc: 0.9889 - val_loss: 3.1424 - val_acc: 0.6360\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0684 - acc: 0.9864 - val_loss: 3.1542 - val_acc: 0.6200\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0658 - acc: 0.9876 - val_loss: 3.1708 - val_acc: 0.6200\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0685 - acc: 0.9864 - val_loss: 3.0781 - val_acc: 0.6160\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0765 - acc: 0.9851 - val_loss: 3.3499 - val_acc: 0.5900\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0843 - acc: 0.9833 - val_loss: 3.1993 - val_acc: 0.6080\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 11.6045 - acc: 0.2446 - val_loss: 10.6927 - val_acc: 0.3060\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 10.3009 - acc: 0.3307 - val_loss: 10.0582 - val_acc: 0.3400\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.6606 - acc: 0.3765 - val_loss: 11.1629 - val_acc: 0.2880\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.6337 - acc: 0.3796 - val_loss: 10.6229 - val_acc: 0.3240\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.2700 - acc: 0.4062 - val_loss: 10.1316 - val_acc: 0.3500\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.0085 - acc: 0.4223 - val_loss: 10.0486 - val_acc: 0.3500\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.8358 - acc: 0.4415 - val_loss: 9.9168 - val_acc: 0.3660\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.0252 - acc: 0.4241 - val_loss: 10.1937 - val_acc: 0.3420\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.0738 - acc: 0.4223 - val_loss: 10.7115 - val_acc: 0.3160\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.9646 - acc: 0.4303 - val_loss: 9.9019 - val_acc: 0.3580\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6371 - acc: 0.4557 - val_loss: 10.0788 - val_acc: 0.3520\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.5388 - acc: 0.4588 - val_loss: 9.8859 - val_acc: 0.3680\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.5987 - acc: 0.4601 - val_loss: 9.8816 - val_acc: 0.3640\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6555 - acc: 0.4508 - val_loss: 10.0926 - val_acc: 0.3520\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3887 - acc: 0.4712 - val_loss: 9.7790 - val_acc: 0.3620\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3442 - acc: 0.4762 - val_loss: 10.2467 - val_acc: 0.3420\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3846 - acc: 0.4724 - val_loss: 9.8239 - val_acc: 0.3760\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6079 - acc: 0.4576 - val_loss: 10.3185 - val_acc: 0.3500\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4962 - acc: 0.4632 - val_loss: 9.9228 - val_acc: 0.3740\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6172 - acc: 0.4533 - val_loss: 9.8490 - val_acc: 0.3740\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4814 - acc: 0.4644 - val_loss: 9.9030 - val_acc: 0.3740\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.7858 - acc: 0.4464 - val_loss: 9.7966 - val_acc: 0.3680\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6467 - acc: 0.4563 - val_loss: 9.7777 - val_acc: 0.3840\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3658 - acc: 0.4762 - val_loss: 9.8276 - val_acc: 0.3780\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3656 - acc: 0.4755 - val_loss: 9.7679 - val_acc: 0.3800\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3449 - acc: 0.4762 - val_loss: 10.0860 - val_acc: 0.3640\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4154 - acc: 0.4712 - val_loss: 9.8486 - val_acc: 0.3740\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4200 - acc: 0.4712 - val_loss: 9.9361 - val_acc: 0.3780\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3192 - acc: 0.4786 - val_loss: 9.8194 - val_acc: 0.3760\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2338 - acc: 0.4848 - val_loss: 9.7924 - val_acc: 0.3800\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2599 - acc: 0.4848 - val_loss: 9.8226 - val_acc: 0.3800\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2893 - acc: 0.4799 - val_loss: 9.7862 - val_acc: 0.3860\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2244 - acc: 0.4867 - val_loss: 9.7100 - val_acc: 0.3920\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2453 - acc: 0.4861 - val_loss: 10.0080 - val_acc: 0.3660\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2791 - acc: 0.4842 - val_loss: 9.8556 - val_acc: 0.3820\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2777 - acc: 0.4848 - val_loss: 9.8041 - val_acc: 0.3820\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2820 - acc: 0.4836 - val_loss: 9.7569 - val_acc: 0.3840\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2488 - acc: 0.4861 - val_loss: 9.9757 - val_acc: 0.3720\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2029 - acc: 0.4898 - val_loss: 9.7314 - val_acc: 0.3820\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2012 - acc: 0.4885 - val_loss: 9.9137 - val_acc: 0.3760\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3389 - acc: 0.4786 - val_loss: 9.7713 - val_acc: 0.3900\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2303 - acc: 0.4848 - val_loss: 9.7833 - val_acc: 0.3860\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2728 - acc: 0.4842 - val_loss: 9.8374 - val_acc: 0.3780\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.1908 - acc: 0.4898 - val_loss: 9.8656 - val_acc: 0.3740\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.1752 - acc: 0.4923 - val_loss: 9.5960 - val_acc: 0.3900\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3006 - acc: 0.4830 - val_loss: 9.9418 - val_acc: 0.3760\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2466 - acc: 0.4836 - val_loss: 9.7301 - val_acc: 0.3840\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2638 - acc: 0.4854 - val_loss: 10.2434 - val_acc: 0.3540\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2532 - acc: 0.4867 - val_loss: 9.7463 - val_acc: 0.3800\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2810 - acc: 0.4830 - val_loss: 9.8309 - val_acc: 0.3800\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_76 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 9.5574 - acc: 0.2644 - val_loss: 7.6001 - val_acc: 0.2420\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.1548 - acc: 0.2824 - val_loss: 7.5921 - val_acc: 0.2640\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.8479 - acc: 0.2867 - val_loss: 7.6243 - val_acc: 0.2840\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3995 - acc: 0.3053 - val_loss: 6.6438 - val_acc: 0.3340\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8564 - acc: 0.3455 - val_loss: 6.1027 - val_acc: 0.3540\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.5126 - acc: 0.3443 - val_loss: 5.9037 - val_acc: 0.3560\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.9956 - acc: 0.3672 - val_loss: 5.5525 - val_acc: 0.3760\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.6716 - acc: 0.3901 - val_loss: 5.2104 - val_acc: 0.3820\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.2492 - acc: 0.4124 - val_loss: 4.8165 - val_acc: 0.4300\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.9340 - acc: 0.4248 - val_loss: 4.5933 - val_acc: 0.4440\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.7346 - acc: 0.4477 - val_loss: 4.7982 - val_acc: 0.4280\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.3857 - acc: 0.4632 - val_loss: 4.3777 - val_acc: 0.4400\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.7841 - acc: 0.5046 - val_loss: 4.2403 - val_acc: 0.4660\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.7604 - acc: 0.4966 - val_loss: 4.4173 - val_acc: 0.4460\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.6891 - acc: 0.5251 - val_loss: 4.2494 - val_acc: 0.4800\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.3524 - acc: 0.5214 - val_loss: 4.0925 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1982 - acc: 0.5486 - val_loss: 3.9646 - val_acc: 0.4960\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.9493 - acc: 0.5610 - val_loss: 3.7755 - val_acc: 0.4920\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.8373 - acc: 0.5709 - val_loss: 3.5517 - val_acc: 0.5160\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.6987 - acc: 0.5839 - val_loss: 3.4738 - val_acc: 0.5300\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.5171 - acc: 0.5957 - val_loss: 3.7331 - val_acc: 0.5160\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.3829 - acc: 0.5950 - val_loss: 3.4109 - val_acc: 0.5200\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.3252 - acc: 0.6087 - val_loss: 3.3449 - val_acc: 0.5320\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.9678 - acc: 0.6334 - val_loss: 3.3318 - val_acc: 0.5500\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7209 - acc: 0.6607 - val_loss: 3.2437 - val_acc: 0.5700\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7379 - acc: 0.6495 - val_loss: 3.0229 - val_acc: 0.5640\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.5876 - acc: 0.6762 - val_loss: 3.0860 - val_acc: 0.5780\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.6348 - acc: 0.6675 - val_loss: 2.8862 - val_acc: 0.5700\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.4222 - acc: 0.6799 - val_loss: 2.8667 - val_acc: 0.5720\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.3245 - acc: 0.6774 - val_loss: 2.9608 - val_acc: 0.5680\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1389 - acc: 0.7009 - val_loss: 2.8071 - val_acc: 0.5820\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.0374 - acc: 0.7102 - val_loss: 2.7731 - val_acc: 0.5960\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.0438 - acc: 0.7015 - val_loss: 2.8245 - val_acc: 0.5980\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.9826 - acc: 0.7245 - val_loss: 2.8218 - val_acc: 0.5840\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.9731 - acc: 0.7164 - val_loss: 2.7765 - val_acc: 0.5820\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.8981 - acc: 0.7127 - val_loss: 2.7733 - val_acc: 0.6040\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.7985 - acc: 0.7325 - val_loss: 2.7223 - val_acc: 0.5960\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6197 - acc: 0.7498 - val_loss: 2.5910 - val_acc: 0.6120\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6371 - acc: 0.7523 - val_loss: 2.6073 - val_acc: 0.5980\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6676 - acc: 0.7449 - val_loss: 2.5651 - val_acc: 0.6220\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4348 - acc: 0.7728 - val_loss: 2.6280 - val_acc: 0.6020\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4206 - acc: 0.7511 - val_loss: 2.6627 - val_acc: 0.5880\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4455 - acc: 0.7635 - val_loss: 2.6253 - val_acc: 0.6080\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4056 - acc: 0.7678 - val_loss: 2.5165 - val_acc: 0.6300\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2715 - acc: 0.7851 - val_loss: 2.6405 - val_acc: 0.6120\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3811 - acc: 0.7765 - val_loss: 2.5276 - val_acc: 0.6160\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3470 - acc: 0.7659 - val_loss: 2.5191 - val_acc: 0.6080\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2546 - acc: 0.7889 - val_loss: 2.5311 - val_acc: 0.6100\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1177 - acc: 0.8192 - val_loss: 2.5496 - val_acc: 0.6040\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2161 - acc: 0.7820 - val_loss: 2.5664 - val_acc: 0.6120\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_77 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_77 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 9.9265 - acc: 0.2817 - val_loss: 9.9852 - val_acc: 0.3240\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.5720 - acc: 0.3201 - val_loss: 8.7126 - val_acc: 0.3360\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2510 - acc: 0.3851 - val_loss: 7.7849 - val_acc: 0.4100\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.1778 - acc: 0.4508 - val_loss: 7.3235 - val_acc: 0.4240\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8676 - acc: 0.4644 - val_loss: 7.6380 - val_acc: 0.4000\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.4829 - acc: 0.5034 - val_loss: 7.1423 - val_acc: 0.4540\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.8528 - acc: 0.5560 - val_loss: 6.8118 - val_acc: 0.4540\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.7984 - acc: 0.5455 - val_loss: 6.6837 - val_acc: 0.4760\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.5041 - acc: 0.5678 - val_loss: 4.2485 - val_acc: 0.5460\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2974 - acc: 0.6582 - val_loss: 3.5707 - val_acc: 0.6000\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8428 - acc: 0.6848 - val_loss: 3.4891 - val_acc: 0.6080\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.2145 - acc: 0.7486 - val_loss: 3.3226 - val_acc: 0.6140\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1891 - acc: 0.7313 - val_loss: 3.2689 - val_acc: 0.6420\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.9934 - acc: 0.7480 - val_loss: 3.3859 - val_acc: 0.6100\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6488 - acc: 0.7858 - val_loss: 3.4683 - val_acc: 0.6060\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6420 - acc: 0.7759 - val_loss: 3.1317 - val_acc: 0.6280\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2838 - acc: 0.8229 - val_loss: 3.3956 - val_acc: 0.6040\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4353 - acc: 0.8050 - val_loss: 3.0467 - val_acc: 0.6340\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0884 - acc: 0.8402 - val_loss: 2.8332 - val_acc: 0.6580\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9863 - acc: 0.8458 - val_loss: 2.9079 - val_acc: 0.6580\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9093 - acc: 0.8526 - val_loss: 2.8810 - val_acc: 0.6440\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7983 - acc: 0.8594 - val_loss: 2.8761 - val_acc: 0.6580\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7268 - acc: 0.8681 - val_loss: 3.0178 - val_acc: 0.6360\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.8478 - acc: 0.8557 - val_loss: 2.7236 - val_acc: 0.6700\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7160 - acc: 0.8762 - val_loss: 2.6367 - val_acc: 0.6520\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6920 - acc: 0.8842 - val_loss: 2.8032 - val_acc: 0.6720\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6176 - acc: 0.8830 - val_loss: 2.7894 - val_acc: 0.6440\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6303 - acc: 0.8997 - val_loss: 2.7777 - val_acc: 0.6580\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5540 - acc: 0.8985 - val_loss: 2.7054 - val_acc: 0.6720\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5118 - acc: 0.9102 - val_loss: 2.6990 - val_acc: 0.6860\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3751 - acc: 0.9195 - val_loss: 2.7173 - val_acc: 0.6640\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4937 - acc: 0.9115 - val_loss: 2.7517 - val_acc: 0.6760\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4576 - acc: 0.9121 - val_loss: 2.9385 - val_acc: 0.6520\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3811 - acc: 0.9170 - val_loss: 2.9449 - val_acc: 0.6620\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4037 - acc: 0.9269 - val_loss: 2.7662 - val_acc: 0.6900\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4285 - acc: 0.9170 - val_loss: 2.8085 - val_acc: 0.6640\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4464 - acc: 0.9133 - val_loss: 2.8612 - val_acc: 0.6720\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3909 - acc: 0.9251 - val_loss: 2.7679 - val_acc: 0.6620\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3341 - acc: 0.9263 - val_loss: 2.7279 - val_acc: 0.6840\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2338 - acc: 0.9467 - val_loss: 2.8067 - val_acc: 0.6700\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3256 - acc: 0.9288 - val_loss: 2.8360 - val_acc: 0.6600\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2807 - acc: 0.9430 - val_loss: 2.7268 - val_acc: 0.6780\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2743 - acc: 0.9325 - val_loss: 2.8782 - val_acc: 0.6740\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2963 - acc: 0.9381 - val_loss: 2.8552 - val_acc: 0.6700\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3946 - acc: 0.9183 - val_loss: 3.1131 - val_acc: 0.6340\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3135 - acc: 0.9375 - val_loss: 3.0431 - val_acc: 0.6480\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3583 - acc: 0.9251 - val_loss: 2.9613 - val_acc: 0.6600\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1686 - acc: 0.9542 - val_loss: 2.8180 - val_acc: 0.6760\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2460 - acc: 0.9517 - val_loss: 2.7621 - val_acc: 0.6920\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1833 - acc: 0.9610 - val_loss: 2.7879 - val_acc: 0.6820\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_78 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_78 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 11.2101 - acc: 0.2563 - val_loss: 9.7606 - val_acc: 0.3000\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 10.7098 - acc: 0.2799 - val_loss: 9.0967 - val_acc: 0.3560\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 10.0564 - acc: 0.3325 - val_loss: 10.4753 - val_acc: 0.3040\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.7942 - acc: 0.3851 - val_loss: 8.6946 - val_acc: 0.3840\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4011 - acc: 0.4254 - val_loss: 8.6358 - val_acc: 0.3940\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.6755 - acc: 0.4681 - val_loss: 8.5239 - val_acc: 0.4160\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0584 - acc: 0.5121 - val_loss: 7.9985 - val_acc: 0.4160\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0347 - acc: 0.5189 - val_loss: 8.2086 - val_acc: 0.4380\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5791 - acc: 0.5418 - val_loss: 8.0504 - val_acc: 0.4260\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8463 - acc: 0.5189 - val_loss: 7.8246 - val_acc: 0.4460\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.2807 - acc: 0.5610 - val_loss: 8.3997 - val_acc: 0.4180\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.3326 - acc: 0.5585 - val_loss: 8.2070 - val_acc: 0.4480\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.3736 - acc: 0.5610 - val_loss: 8.1120 - val_acc: 0.4420\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.2806 - acc: 0.5690 - val_loss: 8.7521 - val_acc: 0.4000\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.0992 - acc: 0.5808 - val_loss: 8.0433 - val_acc: 0.4340\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.8571 - acc: 0.5994 - val_loss: 8.1390 - val_acc: 0.4480\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.6615 - acc: 0.6093 - val_loss: 8.1086 - val_acc: 0.4480\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.4623 - acc: 0.6272 - val_loss: 8.4656 - val_acc: 0.4280\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.9558 - acc: 0.5833 - val_loss: 8.7267 - val_acc: 0.4320\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.7601 - acc: 0.6043 - val_loss: 8.1921 - val_acc: 0.4600\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.7578 - acc: 0.6074 - val_loss: 8.0153 - val_acc: 0.4480\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.8092 - acc: 0.6111 - val_loss: 8.4826 - val_acc: 0.4300\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.4263 - acc: 0.6279 - val_loss: 7.8049 - val_acc: 0.4660\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.5378 - acc: 0.6254 - val_loss: 8.1107 - val_acc: 0.4520\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.5818 - acc: 0.6223 - val_loss: 7.9097 - val_acc: 0.4600\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.1380 - acc: 0.6576 - val_loss: 7.8411 - val_acc: 0.4640\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.2435 - acc: 0.6514 - val_loss: 7.7178 - val_acc: 0.4900\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.0867 - acc: 0.6607 - val_loss: 7.8234 - val_acc: 0.4680\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1087 - acc: 0.6848 - val_loss: 7.3364 - val_acc: 0.4900\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1378 - acc: 0.6879 - val_loss: 6.6547 - val_acc: 0.5200\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.8056 - acc: 0.7226 - val_loss: 6.3578 - val_acc: 0.5400\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.7119 - acc: 0.7232 - val_loss: 5.5721 - val_acc: 0.5860\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1246 - acc: 0.7628 - val_loss: 5.6541 - val_acc: 0.6120\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.3522 - acc: 0.7467 - val_loss: 5.9205 - val_acc: 0.5700\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1616 - acc: 0.7715 - val_loss: 5.9786 - val_acc: 0.5800\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2347 - acc: 0.7647 - val_loss: 5.6032 - val_acc: 0.5920\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.0504 - acc: 0.7752 - val_loss: 6.0290 - val_acc: 0.5780\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7303 - acc: 0.7988 - val_loss: 5.5618 - val_acc: 0.6120\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 2.5773 - acc: 0.8068 - val_loss: 5.8232 - val_acc: 0.5900\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8521 - acc: 0.7907 - val_loss: 5.6731 - val_acc: 0.6140\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.5134 - acc: 0.8080 - val_loss: 5.7841 - val_acc: 0.5980\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7053 - acc: 0.8000 - val_loss: 5.3832 - val_acc: 0.6160\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.3524 - acc: 0.8235 - val_loss: 5.4608 - val_acc: 0.6160\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.3894 - acc: 0.8235 - val_loss: 5.3249 - val_acc: 0.6240\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1637 - acc: 0.8409 - val_loss: 5.4427 - val_acc: 0.6140\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.2516 - acc: 0.8316 - val_loss: 5.5582 - val_acc: 0.6180\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.2221 - acc: 0.8378 - val_loss: 5.5932 - val_acc: 0.6180\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.0965 - acc: 0.8452 - val_loss: 5.1957 - val_acc: 0.6260\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.2192 - acc: 0.8353 - val_loss: 4.9432 - val_acc: 0.6400\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.2354 - acc: 0.8297 - val_loss: 6.0758 - val_acc: 0.5860\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_79 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_79 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 10.2013 - acc: 0.2613 - val_loss: 8.9078 - val_acc: 0.2440\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.6825 - acc: 0.2904 - val_loss: 7.3216 - val_acc: 0.2820\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.9754 - acc: 0.2731 - val_loss: 9.2634 - val_acc: 0.2680\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.5577 - acc: 0.2892 - val_loss: 7.3468 - val_acc: 0.2920\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.2597 - acc: 0.2997 - val_loss: 6.7761 - val_acc: 0.3100\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.2196 - acc: 0.3059 - val_loss: 6.7966 - val_acc: 0.3040\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.8565 - acc: 0.3170 - val_loss: 7.2041 - val_acc: 0.3260\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.6443 - acc: 0.3375 - val_loss: 5.9989 - val_acc: 0.3580\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.1894 - acc: 0.3498 - val_loss: 6.2870 - val_acc: 0.3420\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8994 - acc: 0.3641 - val_loss: 5.5141 - val_acc: 0.3920\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8980 - acc: 0.3672 - val_loss: 5.2062 - val_acc: 0.3960\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.4866 - acc: 0.3870 - val_loss: 5.0082 - val_acc: 0.4280\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.2245 - acc: 0.3957 - val_loss: 5.2712 - val_acc: 0.4240\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8019 - acc: 0.4192 - val_loss: 4.6245 - val_acc: 0.4460\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0088 - acc: 0.4056 - val_loss: 4.9254 - val_acc: 0.4480\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5693 - acc: 0.4452 - val_loss: 4.5806 - val_acc: 0.4700\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.6001 - acc: 0.4303 - val_loss: 4.4807 - val_acc: 0.4680\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.7735 - acc: 0.4421 - val_loss: 4.2292 - val_acc: 0.4840\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.0822 - acc: 0.4644 - val_loss: 4.1745 - val_acc: 0.5060\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.1093 - acc: 0.4762 - val_loss: 4.1234 - val_acc: 0.5140\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.9236 - acc: 0.4786 - val_loss: 4.0290 - val_acc: 0.5340\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.5189 - acc: 0.4947 - val_loss: 4.0168 - val_acc: 0.5220\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.2243 - acc: 0.5189 - val_loss: 3.7637 - val_acc: 0.5420\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.3043 - acc: 0.5183 - val_loss: 3.7244 - val_acc: 0.5500\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.9004 - acc: 0.5319 - val_loss: 3.6877 - val_acc: 0.5580\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.8797 - acc: 0.5288 - val_loss: 3.5615 - val_acc: 0.5600\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.1192 - acc: 0.5164 - val_loss: 3.4963 - val_acc: 0.5600\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.7668 - acc: 0.5307 - val_loss: 3.4499 - val_acc: 0.5840\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.5863 - acc: 0.5573 - val_loss: 3.3843 - val_acc: 0.5700\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.5191 - acc: 0.5430 - val_loss: 3.4174 - val_acc: 0.5700\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.3718 - acc: 0.5690 - val_loss: 3.3403 - val_acc: 0.5820\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.2429 - acc: 0.5746 - val_loss: 3.3444 - val_acc: 0.6020\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.9983 - acc: 0.6012 - val_loss: 3.2301 - val_acc: 0.6000\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.1255 - acc: 0.5765 - val_loss: 3.2181 - val_acc: 0.6100\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.2187 - acc: 0.5882 - val_loss: 3.2022 - val_acc: 0.6060\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.6864 - acc: 0.6105 - val_loss: 3.0686 - val_acc: 0.6040\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.6555 - acc: 0.6074 - val_loss: 3.1925 - val_acc: 0.5960\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.8182 - acc: 0.6211 - val_loss: 3.0646 - val_acc: 0.6000\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.7055 - acc: 0.6161 - val_loss: 2.9841 - val_acc: 0.6140\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.5365 - acc: 0.6198 - val_loss: 2.9826 - val_acc: 0.6180\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.5225 - acc: 0.6229 - val_loss: 2.8493 - val_acc: 0.6140\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.3944 - acc: 0.6223 - val_loss: 2.8881 - val_acc: 0.6120\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2810 - acc: 0.6489 - val_loss: 2.9256 - val_acc: 0.6120\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2307 - acc: 0.6433 - val_loss: 2.8361 - val_acc: 0.6260\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1329 - acc: 0.6489 - val_loss: 2.8169 - val_acc: 0.6220\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.0812 - acc: 0.6570 - val_loss: 2.8760 - val_acc: 0.6260\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1343 - acc: 0.6489 - val_loss: 2.9015 - val_acc: 0.6380\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8480 - acc: 0.6824 - val_loss: 2.6832 - val_acc: 0.6340\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8596 - acc: 0.6749 - val_loss: 2.7234 - val_acc: 0.6400\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.6745 - acc: 0.6954 - val_loss: 2.8207 - val_acc: 0.6340\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_80 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_80 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 10.3270 - acc: 0.2650 - val_loss: 9.1164 - val_acc: 0.2960\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.8450 - acc: 0.2885 - val_loss: 8.5297 - val_acc: 0.3140\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.8233 - acc: 0.3344 - val_loss: 6.3405 - val_acc: 0.3720\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8803 - acc: 0.3759 - val_loss: 6.3037 - val_acc: 0.4140\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.4756 - acc: 0.4080 - val_loss: 6.6050 - val_acc: 0.3920\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5673 - acc: 0.4681 - val_loss: 5.6466 - val_acc: 0.4800\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.1741 - acc: 0.4947 - val_loss: 4.4251 - val_acc: 0.5100\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.7184 - acc: 0.5189 - val_loss: 3.9575 - val_acc: 0.5820\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.1839 - acc: 0.5443 - val_loss: 3.7251 - val_acc: 0.5880\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.6883 - acc: 0.5895 - val_loss: 4.0234 - val_acc: 0.5960\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.6778 - acc: 0.5938 - val_loss: 4.3478 - val_acc: 0.5660\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.4629 - acc: 0.5963 - val_loss: 3.6798 - val_acc: 0.6080\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 4.4429 - acc: 0.6019 - val_loss: 4.2073 - val_acc: 0.5940\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.8980 - acc: 0.6359 - val_loss: 3.4255 - val_acc: 0.6340\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.6080 - acc: 0.6520 - val_loss: 3.3315 - val_acc: 0.6380\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2528 - acc: 0.6811 - val_loss: 3.5988 - val_acc: 0.6220\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.1646 - acc: 0.6762 - val_loss: 3.1796 - val_acc: 0.6340\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 3.2127 - acc: 0.6898 - val_loss: 3.4292 - val_acc: 0.6200\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.8267 - acc: 0.6954 - val_loss: 3.1178 - val_acc: 0.6420\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.6642 - acc: 0.7164 - val_loss: 3.1954 - val_acc: 0.6460\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.7752 - acc: 0.7139 - val_loss: 3.1715 - val_acc: 0.6580\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.5338 - acc: 0.7238 - val_loss: 2.9937 - val_acc: 0.6400\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1813 - acc: 0.7486 - val_loss: 3.3358 - val_acc: 0.6300\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1949 - acc: 0.7511 - val_loss: 3.2094 - val_acc: 0.6600\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1016 - acc: 0.7536 - val_loss: 3.1550 - val_acc: 0.6440\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.1611 - acc: 0.7573 - val_loss: 3.1813 - val_acc: 0.6420\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 2.0985 - acc: 0.7659 - val_loss: 3.0764 - val_acc: 0.6360\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.7536 - acc: 0.7771 - val_loss: 3.1647 - val_acc: 0.6420\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.7966 - acc: 0.7796 - val_loss: 2.9153 - val_acc: 0.6380\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6401 - acc: 0.7957 - val_loss: 2.7752 - val_acc: 0.6580\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6076 - acc: 0.7851 - val_loss: 2.9823 - val_acc: 0.6480\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4958 - acc: 0.8025 - val_loss: 3.0619 - val_acc: 0.6480\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4921 - acc: 0.8087 - val_loss: 2.8147 - val_acc: 0.6440\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.4635 - acc: 0.8111 - val_loss: 2.9129 - val_acc: 0.6440\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3113 - acc: 0.8124 - val_loss: 2.7391 - val_acc: 0.6700\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3565 - acc: 0.8118 - val_loss: 2.8140 - val_acc: 0.6800\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2786 - acc: 0.8285 - val_loss: 2.9261 - val_acc: 0.6440\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2455 - acc: 0.8303 - val_loss: 2.9661 - val_acc: 0.6460\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3399 - acc: 0.8155 - val_loss: 2.8879 - val_acc: 0.6560\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2182 - acc: 0.8241 - val_loss: 2.7350 - val_acc: 0.6540\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2104 - acc: 0.8272 - val_loss: 2.7970 - val_acc: 0.6640\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3133 - acc: 0.8118 - val_loss: 2.6416 - val_acc: 0.6880\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2140 - acc: 0.8310 - val_loss: 2.7071 - val_acc: 0.6580\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0278 - acc: 0.8390 - val_loss: 2.7523 - val_acc: 0.6600\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9196 - acc: 0.8464 - val_loss: 2.6280 - val_acc: 0.6740\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0173 - acc: 0.8576 - val_loss: 2.8154 - val_acc: 0.6560\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9603 - acc: 0.8563 - val_loss: 2.5585 - val_acc: 0.6700\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.8769 - acc: 0.8644 - val_loss: 2.7159 - val_acc: 0.6640\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9660 - acc: 0.8545 - val_loss: 2.8254 - val_acc: 0.6840\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0149 - acc: 0.8489 - val_loss: 2.5969 - val_acc: 0.6840\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_81 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_81 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,579\n",
      "Trainable params: 39,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 11.7297 - acc: 0.2495 - val_loss: 11.5118 - val_acc: 0.2840\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 11.2229 - acc: 0.2718 - val_loss: 10.5233 - val_acc: 0.3040\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 10.3900 - acc: 0.3108 - val_loss: 8.1674 - val_acc: 0.4060\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 10.0090 - acc: 0.3449 - val_loss: 9.3716 - val_acc: 0.3860\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.5627 - acc: 0.3647 - val_loss: 8.1872 - val_acc: 0.4340\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.9268 - acc: 0.4111 - val_loss: 8.6137 - val_acc: 0.4200\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.7114 - acc: 0.4248 - val_loss: 8.1763 - val_acc: 0.4440\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.5982 - acc: 0.4334 - val_loss: 8.6677 - val_acc: 0.4140\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.4100 - acc: 0.4415 - val_loss: 8.5478 - val_acc: 0.4320\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.3001 - acc: 0.4576 - val_loss: 7.7950 - val_acc: 0.4680\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.0818 - acc: 0.4650 - val_loss: 7.5393 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.5237 - acc: 0.4477 - val_loss: 8.1651 - val_acc: 0.4620\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2495 - acc: 0.4656 - val_loss: 8.2920 - val_acc: 0.4660\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.1725 - acc: 0.4687 - val_loss: 7.8119 - val_acc: 0.4940\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 8.2257 - acc: 0.4669 - val_loss: 7.7133 - val_acc: 0.4980\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8798 - acc: 0.4910 - val_loss: 7.8392 - val_acc: 0.4780\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.5595 - acc: 0.5108 - val_loss: 7.8092 - val_acc: 0.4700\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.5723 - acc: 0.5084 - val_loss: 7.7875 - val_acc: 0.4860\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.5130 - acc: 0.5115 - val_loss: 7.6112 - val_acc: 0.5120\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 9.0409 - acc: 0.4211 - val_loss: 8.4173 - val_acc: 0.4580\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 7.8179 - acc: 0.4972 - val_loss: 7.4105 - val_acc: 0.5200\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.3049 - acc: 0.5294 - val_loss: 7.0753 - val_acc: 0.5400\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.8768 - acc: 0.4904 - val_loss: 6.9409 - val_acc: 0.5480\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.2649 - acc: 0.5307 - val_loss: 7.0666 - val_acc: 0.5300\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0268 - acc: 0.5486 - val_loss: 7.0079 - val_acc: 0.5440\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0670 - acc: 0.5437 - val_loss: 7.1844 - val_acc: 0.5280\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.0107 - acc: 0.5498 - val_loss: 7.0561 - val_acc: 0.5400\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.7352 - acc: 0.5573 - val_loss: 7.0015 - val_acc: 0.5500\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8763 - acc: 0.5585 - val_loss: 7.0976 - val_acc: 0.5380\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 7.1022 - acc: 0.5455 - val_loss: 7.9918 - val_acc: 0.4780\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.9693 - acc: 0.5523 - val_loss: 7.2063 - val_acc: 0.5360\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 6.9519 - acc: 0.5554 - val_loss: 7.1270 - val_acc: 0.5300\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.9000 - acc: 0.5554 - val_loss: 7.1412 - val_acc: 0.5320\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.1261 - acc: 0.5480 - val_loss: 6.9933 - val_acc: 0.5500\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.6750 - acc: 0.5672 - val_loss: 7.6815 - val_acc: 0.5120\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 6.8394 - acc: 0.5641 - val_loss: 6.7045 - val_acc: 0.5660\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8170 - acc: 0.5690 - val_loss: 6.9255 - val_acc: 0.5540\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.6858 - acc: 0.5703 - val_loss: 7.7042 - val_acc: 0.5080\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 7.2317 - acc: 0.5350 - val_loss: 7.5338 - val_acc: 0.5060\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 7.1024 - acc: 0.5467 - val_loss: 7.1325 - val_acc: 0.5400\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.7343 - acc: 0.5678 - val_loss: 7.0259 - val_acc: 0.5520\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.7232 - acc: 0.5709 - val_loss: 6.9895 - val_acc: 0.5460\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8167 - acc: 0.5604 - val_loss: 7.3379 - val_acc: 0.5240\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.8809 - acc: 0.5604 - val_loss: 6.9777 - val_acc: 0.5440\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5747 - acc: 0.5802 - val_loss: 7.5970 - val_acc: 0.5020\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.6814 - acc: 0.5721 - val_loss: 7.0992 - val_acc: 0.5400\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5936 - acc: 0.5709 - val_loss: 6.8825 - val_acc: 0.5620\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 6.5694 - acc: 0.5752 - val_loss: 6.3485 - val_acc: 0.5880\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 6s 4ms/sample - loss: 5.8262 - acc: 0.6186 - val_loss: 6.1947 - val_acc: 0.5860\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 5.5031 - acc: 0.6384 - val_loss: 6.5448 - val_acc: 0.5700\n",
      "Optimal learning rate: 0.0005 Optimal dropout rate: 0.7\n",
      "\n",
      " Validation accuracy overall w dropout w/o bn: 0.684\n",
      "\n",
      " Test accuracy overall: 0.66591424\n",
      "Test accuracy person 0 0.62\n",
      "Test accuracy person 1 0.64\n",
      "Test accuracy person 2 0.46\n",
      "Test accuracy person 3 0.68\n",
      "Test accuracy person 4 0.87234044\n",
      "Test accuracy person 5 0.53061223\n",
      "Test accuracy person 6 0.82\n",
      "Test accuracy person 7 0.74\n",
      "Test accuracy person 8 0.63829786\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.2, 0.5, 0.7]\n",
    "learning_rate_list = [1e-4, 5e-4, 1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "for dropout in dropout_rate_list:\n",
    "  for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=50,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters = [dropout, learning_rate]\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "print (\"Optimal learning rate: \" +str(parameters[1]) +  \" Optimal dropout rate: \" +str(parameters[0]))\n",
    "\n",
    "score = model_max.evaluate(x_valid, y_valid, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Validation accuracy overall w dropout w/o bn:', score[1]) \n",
    "score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "for i in range(9):    \n",
    "  score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "  print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvaTH0KpaG5S"
   },
   "source": [
    "### **Shallow CNN with dropout and w/o batch normalization (SCNN-dp): Multiple Runs**\n",
    "\n",
    "We run SCNN-dp with optimal learning rate selected in the previous block multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "Ypk9OvwnS9CZ",
    "outputId": "a06e0f00-fa30-4481-c1dc-e9c911e1caf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy overall w/o bn w dropout: 0.4916000008583069 with sd 0.12360842326321003\n",
      "Validation accuracy persons w/o bn w dropout: [0.43230771 0.46567165 0.43833333 0.43703704 0.57307693 0.48113207\n",
      " 0.60222223 0.57777778 0.46      ] with sd [0.09909652 0.13161438 0.08883505 0.10579894 0.15712845 0.13321609\n",
      " 0.19485988 0.16308918 0.1353514 ]\n",
      "Test accuracy overall w/o bn w dropout: 0.5137697517871856 with sd 0.12477251898473828\n",
      "Test accuracy persons w/o bn w dropout: [0.428      0.512      0.436      0.486      0.64680851 0.4755102\n",
      " 0.614      0.53599999 0.49574468] with sd [0.11668762 0.13212116 0.08475848 0.18309561 0.17907767 0.09354422\n",
      " 0.14860687 0.17995555 0.13048296]\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.5]\n",
    "learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "N_iter             = 10\n",
    "score_arr          = np.zeros(N_iter)\n",
    "score_person       = np.zeros((9,N_iter))\n",
    "score_arr_valid          = np.zeros(N_iter)\n",
    "score_person_valid       = np.zeros((9,N_iter))\n",
    "for n in range(N_iter):\n",
    "  for dropout in dropout_rate_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "      model.add(tf.keras.layers.Dropout(dropout))\n",
    "      model.add(tf.keras.layers.Flatten())\n",
    "      model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "      \n",
    "      optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "      model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=['accuracy'])\n",
    "  #     model.summary()\n",
    "      model.fit(x_train,\n",
    "               y_train,\n",
    "               batch_size=64,\n",
    "               epochs=50,\n",
    "               validation_data=(x_valid, y_valid), verbose=False)\n",
    "      score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "\n",
    "  score_arr_valid[n] = model.evaluate(x_valid, y_valid, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person_valid[i,n] = model.evaluate(x_valid[person_train_valid[ind_valid].T[0]==i], y_valid[person_train_valid[ind_valid].T[0]==i], verbose=0)[1]        \n",
    "\n",
    "\n",
    "  score_arr[n] = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person[i,n] = model.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)[1]    \n",
    "    \n",
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall w/o bn w dropout: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons w/o bn w dropout: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n",
    "\n",
    "score_final_overall = np.mean(score_arr)  \n",
    "score_final_std     = np.std(score_arr)\n",
    "score_final_person        = np.mean(score_person, axis=1)\n",
    "score_final_person_std    = np.std(score_person, axis=1)\n",
    "print('Test accuracy overall w/o bn w dropout: ' +  str(score_final_overall) + ' with sd ' + str(score_final_std))\n",
    "print('Test accuracy persons w/o bn w dropout: ' +  str(score_final_person)+ ' with sd ' + str(score_final_person_std)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNg4czpSMjAd"
   },
   "source": [
    "## **SCNN-dp-bn: Shallow CNN with dropout and with bn.**\n",
    "\n",
    "** Layer 1**:  25 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1)  $\\rightarrow$ BNorm $\\rightarrow$  Dropout $\\rightarrow$ Flatten\n",
    "\n",
    "**Layer 2:** FCNet with 4 outputs.\n",
    "\n",
    "We train the SCNN-dp with optimal hyperparameters (learning rate, dropout) in the block below. In the next block, we fix the learned hyperparameter and run the model multiple times to find mean test accuracy and standard deviation of test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18615
    },
    "colab_type": "code",
    "id": "nbqMqfhYLzzW",
    "outputId": "c470622e-bed4-4d2f-b0a1-b2a1cef770de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_82 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_82 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 1.9874 - acc: 0.2737 - val_loss: 2.8660 - val_acc: 0.2720\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.6489 - acc: 0.3443 - val_loss: 2.1737 - val_acc: 0.3160\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.4180 - acc: 0.4303 - val_loss: 1.8825 - val_acc: 0.3460\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.2527 - acc: 0.4768 - val_loss: 1.7191 - val_acc: 0.3740\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1367 - acc: 0.5214 - val_loss: 1.6118 - val_acc: 0.3880\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0221 - acc: 0.5721 - val_loss: 1.5397 - val_acc: 0.4240\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9397 - acc: 0.6043 - val_loss: 1.4861 - val_acc: 0.4440\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8469 - acc: 0.6644 - val_loss: 1.4383 - val_acc: 0.4660\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7822 - acc: 0.6898 - val_loss: 1.3996 - val_acc: 0.4940\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7309 - acc: 0.7096 - val_loss: 1.3660 - val_acc: 0.4960\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6643 - acc: 0.7344 - val_loss: 1.3345 - val_acc: 0.5080\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.6127 - acc: 0.7690 - val_loss: 1.3088 - val_acc: 0.5100\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.5720 - acc: 0.7920 - val_loss: 1.2998 - val_acc: 0.5240\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5313 - acc: 0.8087 - val_loss: 1.2702 - val_acc: 0.5320\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5092 - acc: 0.8204 - val_loss: 1.2560 - val_acc: 0.5280\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4715 - acc: 0.8359 - val_loss: 1.2509 - val_acc: 0.5340\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4282 - acc: 0.8681 - val_loss: 1.2355 - val_acc: 0.5380\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.4183 - acc: 0.8681 - val_loss: 1.2340 - val_acc: 0.5420\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3883 - acc: 0.8817 - val_loss: 1.2314 - val_acc: 0.5460\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3804 - acc: 0.8774 - val_loss: 1.2291 - val_acc: 0.5520\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3485 - acc: 0.9090 - val_loss: 1.2295 - val_acc: 0.5480\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3279 - acc: 0.9115 - val_loss: 1.2245 - val_acc: 0.5600\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3078 - acc: 0.9040 - val_loss: 1.2165 - val_acc: 0.5720\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2894 - acc: 0.9263 - val_loss: 1.2137 - val_acc: 0.5680\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2821 - acc: 0.9288 - val_loss: 1.2179 - val_acc: 0.5740\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2762 - acc: 0.9282 - val_loss: 1.2206 - val_acc: 0.5800\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2530 - acc: 0.9467 - val_loss: 1.2283 - val_acc: 0.5840\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2471 - acc: 0.9443 - val_loss: 1.2206 - val_acc: 0.5800\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2115 - acc: 0.9616 - val_loss: 1.2210 - val_acc: 0.5760\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2199 - acc: 0.9523 - val_loss: 1.2154 - val_acc: 0.5700\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2065 - acc: 0.9628 - val_loss: 1.2204 - val_acc: 0.5780\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1957 - acc: 0.9647 - val_loss: 1.2223 - val_acc: 0.5840\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1896 - acc: 0.9684 - val_loss: 1.2283 - val_acc: 0.5800\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1760 - acc: 0.9690 - val_loss: 1.2231 - val_acc: 0.5860\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1683 - acc: 0.9752 - val_loss: 1.2229 - val_acc: 0.5860\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1560 - acc: 0.9814 - val_loss: 1.2273 - val_acc: 0.5860\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1562 - acc: 0.9808 - val_loss: 1.2307 - val_acc: 0.5900\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1440 - acc: 0.9833 - val_loss: 1.2246 - val_acc: 0.5900\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1421 - acc: 0.9839 - val_loss: 1.2298 - val_acc: 0.5960\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1376 - acc: 0.9827 - val_loss: 1.2356 - val_acc: 0.5920\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1275 - acc: 0.9913 - val_loss: 1.2378 - val_acc: 0.5940\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1262 - acc: 0.9876 - val_loss: 1.2364 - val_acc: 0.5960\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1151 - acc: 0.9882 - val_loss: 1.2302 - val_acc: 0.6000\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1193 - acc: 0.9876 - val_loss: 1.2411 - val_acc: 0.6040\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1129 - acc: 0.9913 - val_loss: 1.2396 - val_acc: 0.6020\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1102 - acc: 0.9889 - val_loss: 1.2447 - val_acc: 0.6060\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1013 - acc: 0.9926 - val_loss: 1.2514 - val_acc: 0.5960\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0986 - acc: 0.9969 - val_loss: 1.2428 - val_acc: 0.6100\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0975 - acc: 0.9957 - val_loss: 1.2427 - val_acc: 0.6100\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0961 - acc: 0.9932 - val_loss: 1.2530 - val_acc: 0.6040\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_83 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 1.7675 - acc: 0.3325 - val_loss: 2.8567 - val_acc: 0.4140\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9350 - acc: 0.6248 - val_loss: 1.7485 - val_acc: 0.4900\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6553 - acc: 0.7616 - val_loss: 1.4728 - val_acc: 0.5260\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4755 - acc: 0.8316 - val_loss: 1.3419 - val_acc: 0.5340\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3786 - acc: 0.8718 - val_loss: 1.2455 - val_acc: 0.5780\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2973 - acc: 0.9090 - val_loss: 1.2599 - val_acc: 0.5780\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2367 - acc: 0.9337 - val_loss: 1.2515 - val_acc: 0.5800\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1988 - acc: 0.9542 - val_loss: 1.2090 - val_acc: 0.5920\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1574 - acc: 0.9628 - val_loss: 1.1703 - val_acc: 0.5980\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1229 - acc: 0.9876 - val_loss: 1.2018 - val_acc: 0.6040\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1174 - acc: 0.9808 - val_loss: 1.2133 - val_acc: 0.6200\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1021 - acc: 0.9851 - val_loss: 1.2088 - val_acc: 0.6140\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0823 - acc: 0.9920 - val_loss: 1.2405 - val_acc: 0.6060\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0690 - acc: 0.9944 - val_loss: 1.2399 - val_acc: 0.6200\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0589 - acc: 0.9969 - val_loss: 1.2351 - val_acc: 0.6120\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0555 - acc: 0.9981 - val_loss: 1.2599 - val_acc: 0.6160\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0545 - acc: 0.9957 - val_loss: 1.2813 - val_acc: 0.6240\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0452 - acc: 0.9994 - val_loss: 1.2711 - val_acc: 0.6080\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0398 - acc: 0.9975 - val_loss: 1.2981 - val_acc: 0.6180\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0377 - acc: 0.9988 - val_loss: 1.2779 - val_acc: 0.6260\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0312 - acc: 0.9988 - val_loss: 1.3113 - val_acc: 0.6140\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0331 - acc: 0.9994 - val_loss: 1.3252 - val_acc: 0.6240\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0323 - acc: 0.9988 - val_loss: 1.3377 - val_acc: 0.6120\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0264 - acc: 0.9994 - val_loss: 1.3223 - val_acc: 0.6280\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0262 - acc: 0.9988 - val_loss: 1.3804 - val_acc: 0.6240\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0213 - acc: 1.0000 - val_loss: 1.3376 - val_acc: 0.6180\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0214 - acc: 0.9994 - val_loss: 1.3728 - val_acc: 0.6120\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0185 - acc: 0.9994 - val_loss: 1.3779 - val_acc: 0.6160\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0200 - acc: 0.9988 - val_loss: 1.4014 - val_acc: 0.6240\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0183 - acc: 1.0000 - val_loss: 1.4128 - val_acc: 0.6240\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0161 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 0.6260\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0138 - acc: 1.0000 - val_loss: 1.3885 - val_acc: 0.6300\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0159 - acc: 0.9994 - val_loss: 1.3899 - val_acc: 0.6340\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0129 - acc: 1.0000 - val_loss: 1.3809 - val_acc: 0.6300\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0129 - acc: 1.0000 - val_loss: 1.3858 - val_acc: 0.6340\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0114 - acc: 1.0000 - val_loss: 1.4009 - val_acc: 0.6240\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0127 - acc: 1.0000 - val_loss: 1.4234 - val_acc: 0.6320\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0102 - acc: 1.0000 - val_loss: 1.4394 - val_acc: 0.6340\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0097 - acc: 1.0000 - val_loss: 1.4174 - val_acc: 0.6300\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0100 - acc: 1.0000 - val_loss: 1.4154 - val_acc: 0.6400\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0090 - acc: 1.0000 - val_loss: 1.4048 - val_acc: 0.6440\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0091 - acc: 1.0000 - val_loss: 1.4165 - val_acc: 0.6380\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0081 - acc: 1.0000 - val_loss: 1.4215 - val_acc: 0.6320\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0074 - acc: 1.0000 - val_loss: 1.4666 - val_acc: 0.6260\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0073 - acc: 1.0000 - val_loss: 1.4520 - val_acc: 0.6440\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 1.4534 - val_acc: 0.6340\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 1.4977 - val_acc: 0.6140\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0114 - acc: 0.9988 - val_loss: 1.4873 - val_acc: 0.6340\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0091 - acc: 1.0000 - val_loss: 1.5511 - val_acc: 0.6060\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0094 - acc: 0.9994 - val_loss: 1.5401 - val_acc: 0.6180\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_84 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 1.7736 - acc: 0.3697 - val_loss: 2.6292 - val_acc: 0.4600\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7675 - acc: 0.7127 - val_loss: 1.6270 - val_acc: 0.5280\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4955 - acc: 0.8149 - val_loss: 1.5375 - val_acc: 0.5640\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3455 - acc: 0.8755 - val_loss: 1.3692 - val_acc: 0.5780\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2347 - acc: 0.9337 - val_loss: 1.4028 - val_acc: 0.5760\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1700 - acc: 0.9529 - val_loss: 1.3278 - val_acc: 0.6140\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1157 - acc: 0.9752 - val_loss: 1.3415 - val_acc: 0.6220\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0886 - acc: 0.9864 - val_loss: 1.4086 - val_acc: 0.5860\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0709 - acc: 0.9907 - val_loss: 1.3666 - val_acc: 0.6080\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0610 - acc: 0.9926 - val_loss: 1.3754 - val_acc: 0.6180\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0498 - acc: 0.9944 - val_loss: 1.4213 - val_acc: 0.5980\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0408 - acc: 0.9963 - val_loss: 1.4225 - val_acc: 0.5940\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0387 - acc: 0.9944 - val_loss: 1.4303 - val_acc: 0.6180\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0282 - acc: 0.9994 - val_loss: 1.4488 - val_acc: 0.6240\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0311 - acc: 0.9963 - val_loss: 1.4505 - val_acc: 0.6200\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0264 - acc: 0.9981 - val_loss: 1.4791 - val_acc: 0.6120\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0262 - acc: 0.9975 - val_loss: 1.4713 - val_acc: 0.6180\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0305 - acc: 0.9944 - val_loss: 1.4985 - val_acc: 0.6220\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0214 - acc: 1.0000 - val_loss: 1.5067 - val_acc: 0.6200\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0187 - acc: 0.9994 - val_loss: 1.5125 - val_acc: 0.6280\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0154 - acc: 1.0000 - val_loss: 1.5038 - val_acc: 0.6340\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0137 - acc: 1.0000 - val_loss: 1.5280 - val_acc: 0.6360\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0103 - acc: 1.0000 - val_loss: 1.5322 - val_acc: 0.6280\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0131 - acc: 0.9981 - val_loss: 1.5998 - val_acc: 0.6260\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0132 - acc: 0.9988 - val_loss: 1.5691 - val_acc: 0.6440\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0106 - acc: 1.0000 - val_loss: 1.5933 - val_acc: 0.6440\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 1.5526 - val_acc: 0.6440\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0088 - acc: 1.0000 - val_loss: 1.6027 - val_acc: 0.6360\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0073 - acc: 1.0000 - val_loss: 1.5952 - val_acc: 0.6320\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0066 - acc: 1.0000 - val_loss: 1.6062 - val_acc: 0.6380\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0065 - acc: 0.9994 - val_loss: 1.6144 - val_acc: 0.6360\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 1.5978 - val_acc: 0.6440\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 1.6126 - val_acc: 0.6420\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 1.5970 - val_acc: 0.6380\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 1.6109 - val_acc: 0.6460\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 1.6510 - val_acc: 0.6360\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 1.6163 - val_acc: 0.6440\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0082 - acc: 0.9988 - val_loss: 1.7169 - val_acc: 0.6360\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0093 - acc: 0.9994 - val_loss: 1.6835 - val_acc: 0.6440\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 1.7356 - val_acc: 0.6500\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 1.7261 - val_acc: 0.6200\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 1.7706 - val_acc: 0.6260\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0047 - acc: 0.9994 - val_loss: 1.7357 - val_acc: 0.6340\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 1.7890 - val_acc: 0.6400\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 1.7476 - val_acc: 0.6360\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 0.6540\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 1.8211 - val_acc: 0.6300\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0073 - acc: 1.0000 - val_loss: 1.8723 - val_acc: 0.6300\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0111 - acc: 0.9981 - val_loss: 1.9545 - val_acc: 0.6220\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 1.9182 - val_acc: 0.6220\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_85 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_85 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 2.2530 - acc: 0.2793 - val_loss: 2.6688 - val_acc: 0.3300\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.9983 - acc: 0.3387 - val_loss: 2.0128 - val_acc: 0.3740\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.7838 - acc: 0.3759 - val_loss: 1.7363 - val_acc: 0.4020\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.6206 - acc: 0.4272 - val_loss: 1.5501 - val_acc: 0.4000\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.5547 - acc: 0.4539 - val_loss: 1.4484 - val_acc: 0.4400\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3594 - acc: 0.4935 - val_loss: 1.3873 - val_acc: 0.4560\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.2802 - acc: 0.5300 - val_loss: 1.3309 - val_acc: 0.4740\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1951 - acc: 0.5437 - val_loss: 1.3040 - val_acc: 0.4860\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1386 - acc: 0.5684 - val_loss: 1.2663 - val_acc: 0.5060\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.0692 - acc: 0.6031 - val_loss: 1.2368 - val_acc: 0.5160\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9974 - acc: 0.6155 - val_loss: 1.2083 - val_acc: 0.5360\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8972 - acc: 0.6619 - val_loss: 1.1885 - val_acc: 0.5420\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9150 - acc: 0.6613 - val_loss: 1.1753 - val_acc: 0.5520\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8185 - acc: 0.6805 - val_loss: 1.1729 - val_acc: 0.5640\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8127 - acc: 0.6879 - val_loss: 1.1645 - val_acc: 0.5700\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7836 - acc: 0.7015 - val_loss: 1.1617 - val_acc: 0.5700\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.7460 - acc: 0.7127 - val_loss: 1.1431 - val_acc: 0.5760\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7105 - acc: 0.7226 - val_loss: 1.1369 - val_acc: 0.5740\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6763 - acc: 0.7399 - val_loss: 1.1392 - val_acc: 0.5840\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6393 - acc: 0.7474 - val_loss: 1.1413 - val_acc: 0.5880\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6252 - acc: 0.7542 - val_loss: 1.1393 - val_acc: 0.5780\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6096 - acc: 0.7616 - val_loss: 1.1475 - val_acc: 0.5840\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5650 - acc: 0.7907 - val_loss: 1.1359 - val_acc: 0.5880\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5565 - acc: 0.7950 - val_loss: 1.1232 - val_acc: 0.5920\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5596 - acc: 0.7876 - val_loss: 1.1269 - val_acc: 0.5980\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5239 - acc: 0.8037 - val_loss: 1.1269 - val_acc: 0.5960\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5000 - acc: 0.8000 - val_loss: 1.1230 - val_acc: 0.6000\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4908 - acc: 0.8111 - val_loss: 1.1278 - val_acc: 0.5940\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4554 - acc: 0.8254 - val_loss: 1.1227 - val_acc: 0.5840\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4769 - acc: 0.8235 - val_loss: 1.1257 - val_acc: 0.5940\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4269 - acc: 0.8421 - val_loss: 1.1177 - val_acc: 0.6060\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4217 - acc: 0.8489 - val_loss: 1.1175 - val_acc: 0.6160\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4235 - acc: 0.8427 - val_loss: 1.1297 - val_acc: 0.6040\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3935 - acc: 0.8557 - val_loss: 1.1258 - val_acc: 0.6000\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4064 - acc: 0.8495 - val_loss: 1.1167 - val_acc: 0.6100\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3529 - acc: 0.8762 - val_loss: 1.1262 - val_acc: 0.6000\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3652 - acc: 0.8681 - val_loss: 1.1331 - val_acc: 0.6080\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3589 - acc: 0.8663 - val_loss: 1.1364 - val_acc: 0.5960\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3497 - acc: 0.8793 - val_loss: 1.1154 - val_acc: 0.6080\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3586 - acc: 0.8712 - val_loss: 1.1196 - val_acc: 0.6080\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3125 - acc: 0.8861 - val_loss: 1.1272 - val_acc: 0.6100\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2891 - acc: 0.8966 - val_loss: 1.1386 - val_acc: 0.6060\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2942 - acc: 0.8947 - val_loss: 1.1216 - val_acc: 0.6100\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2820 - acc: 0.9090 - val_loss: 1.1251 - val_acc: 0.6020\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2832 - acc: 0.8960 - val_loss: 1.1277 - val_acc: 0.6100\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2774 - acc: 0.9034 - val_loss: 1.1239 - val_acc: 0.6160\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2759 - acc: 0.9053 - val_loss: 1.1299 - val_acc: 0.6040\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2710 - acc: 0.9046 - val_loss: 1.1276 - val_acc: 0.6120\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2733 - acc: 0.9084 - val_loss: 1.1171 - val_acc: 0.6180\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2383 - acc: 0.9214 - val_loss: 1.1091 - val_acc: 0.6220\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_86 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_86 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 2.1891 - acc: 0.2898 - val_loss: 2.6395 - val_acc: 0.3700\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.3695 - acc: 0.4985 - val_loss: 1.9410 - val_acc: 0.4540\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 1.1176 - acc: 0.5827 - val_loss: 1.4778 - val_acc: 0.4740\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.9115 - acc: 0.6563 - val_loss: 1.4443 - val_acc: 0.5280\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7231 - acc: 0.7226 - val_loss: 1.3716 - val_acc: 0.5400\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6495 - acc: 0.7467 - val_loss: 1.2866 - val_acc: 0.5500\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5395 - acc: 0.7950 - val_loss: 1.2910 - val_acc: 0.5340\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4560 - acc: 0.8260 - val_loss: 1.2614 - val_acc: 0.5500\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4092 - acc: 0.8452 - val_loss: 1.2142 - val_acc: 0.5860\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4141 - acc: 0.8440 - val_loss: 1.2138 - val_acc: 0.6020\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.3454 - acc: 0.8669 - val_loss: 1.2092 - val_acc: 0.5900\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3008 - acc: 0.8923 - val_loss: 1.1955 - val_acc: 0.5760\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2749 - acc: 0.9053 - val_loss: 1.2046 - val_acc: 0.5960\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2521 - acc: 0.9158 - val_loss: 1.2057 - val_acc: 0.5900\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2313 - acc: 0.9207 - val_loss: 1.2407 - val_acc: 0.5980\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1971 - acc: 0.9381 - val_loss: 1.2688 - val_acc: 0.5720\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1914 - acc: 0.9381 - val_loss: 1.2921 - val_acc: 0.5960\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2064 - acc: 0.9375 - val_loss: 1.2582 - val_acc: 0.5940\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1728 - acc: 0.9387 - val_loss: 1.2983 - val_acc: 0.5960\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1424 - acc: 0.9554 - val_loss: 1.3251 - val_acc: 0.6000\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1630 - acc: 0.9418 - val_loss: 1.2862 - val_acc: 0.6040\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1265 - acc: 0.9610 - val_loss: 1.2687 - val_acc: 0.6080\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1294 - acc: 0.9548 - val_loss: 1.2751 - val_acc: 0.6020\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.1159 - acc: 0.9616 - val_loss: 1.3126 - val_acc: 0.5960\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1082 - acc: 0.9697 - val_loss: 1.2993 - val_acc: 0.6200\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1140 - acc: 0.9709 - val_loss: 1.3333 - val_acc: 0.6180\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0965 - acc: 0.9709 - val_loss: 1.3678 - val_acc: 0.6100\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0890 - acc: 0.9752 - val_loss: 1.3541 - val_acc: 0.6160\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0927 - acc: 0.9746 - val_loss: 1.3620 - val_acc: 0.6280\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0851 - acc: 0.9759 - val_loss: 1.3523 - val_acc: 0.6320\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0832 - acc: 0.9734 - val_loss: 1.4188 - val_acc: 0.6000\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0857 - acc: 0.9759 - val_loss: 1.4135 - val_acc: 0.6260\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0770 - acc: 0.9789 - val_loss: 1.4229 - val_acc: 0.6160\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.0868 - acc: 0.9759 - val_loss: 1.4439 - val_acc: 0.6220\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 0.0724 - acc: 0.9796 - val_loss: 1.4070 - val_acc: 0.6180\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0680 - acc: 0.9845 - val_loss: 1.4245 - val_acc: 0.6140\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0608 - acc: 0.9808 - val_loss: 1.4552 - val_acc: 0.6160\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0794 - acc: 0.9771 - val_loss: 1.4659 - val_acc: 0.6100\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0552 - acc: 0.9864 - val_loss: 1.4197 - val_acc: 0.6080\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0583 - acc: 0.9858 - val_loss: 1.4604 - val_acc: 0.6140\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0623 - acc: 0.9833 - val_loss: 1.4801 - val_acc: 0.6140\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0623 - acc: 0.9802 - val_loss: 1.4439 - val_acc: 0.6280\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0534 - acc: 0.9858 - val_loss: 1.4694 - val_acc: 0.6200\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0521 - acc: 0.9858 - val_loss: 1.4663 - val_acc: 0.6320\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0376 - acc: 0.9938 - val_loss: 1.4937 - val_acc: 0.6400\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0481 - acc: 0.9839 - val_loss: 1.4793 - val_acc: 0.6260\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0592 - acc: 0.9839 - val_loss: 1.5135 - val_acc: 0.6240\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0576 - acc: 0.9802 - val_loss: 1.5515 - val_acc: 0.6400\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0538 - acc: 0.9827 - val_loss: 1.5177 - val_acc: 0.6360\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0604 - acc: 0.9820 - val_loss: 1.5744 - val_acc: 0.6420\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_87 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 2.1812 - acc: 0.3307 - val_loss: 3.3038 - val_acc: 0.4060\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.1754 - acc: 0.5808 - val_loss: 1.9108 - val_acc: 0.5120\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8648 - acc: 0.6867 - val_loss: 1.5791 - val_acc: 0.5480\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6960 - acc: 0.7536 - val_loss: 1.3466 - val_acc: 0.5940\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5762 - acc: 0.7957 - val_loss: 1.4861 - val_acc: 0.5680\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4692 - acc: 0.8217 - val_loss: 1.4666 - val_acc: 0.5780\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3939 - acc: 0.8551 - val_loss: 1.4587 - val_acc: 0.5860\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3452 - acc: 0.8762 - val_loss: 1.3618 - val_acc: 0.6280\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2799 - acc: 0.8941 - val_loss: 1.3658 - val_acc: 0.6120\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2677 - acc: 0.9077 - val_loss: 1.4872 - val_acc: 0.5940\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2142 - acc: 0.9214 - val_loss: 1.4318 - val_acc: 0.6180\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2209 - acc: 0.9201 - val_loss: 1.5572 - val_acc: 0.6080\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2058 - acc: 0.9282 - val_loss: 1.4929 - val_acc: 0.6140\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1808 - acc: 0.9294 - val_loss: 1.5637 - val_acc: 0.6040\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1589 - acc: 0.9406 - val_loss: 1.5345 - val_acc: 0.6260\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1794 - acc: 0.9399 - val_loss: 1.5480 - val_acc: 0.6220\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1364 - acc: 0.9529 - val_loss: 1.5718 - val_acc: 0.6280\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1534 - acc: 0.9455 - val_loss: 1.6242 - val_acc: 0.6240\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1423 - acc: 0.9567 - val_loss: 1.5293 - val_acc: 0.6300\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1282 - acc: 0.9511 - val_loss: 1.5600 - val_acc: 0.6180\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1173 - acc: 0.9616 - val_loss: 1.5765 - val_acc: 0.6160\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.1220 - acc: 0.9635 - val_loss: 1.6232 - val_acc: 0.6200\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1262 - acc: 0.9542 - val_loss: 1.6069 - val_acc: 0.6320\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1234 - acc: 0.9517 - val_loss: 1.7081 - val_acc: 0.6120\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.1074 - acc: 0.9641 - val_loss: 1.6525 - val_acc: 0.6100\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1131 - acc: 0.9598 - val_loss: 1.8320 - val_acc: 0.5960\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.1237 - acc: 0.9548 - val_loss: 1.7245 - val_acc: 0.6160\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0934 - acc: 0.9697 - val_loss: 1.8010 - val_acc: 0.6120\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0818 - acc: 0.9728 - val_loss: 1.7219 - val_acc: 0.6120\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0902 - acc: 0.9678 - val_loss: 1.7895 - val_acc: 0.6160\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0865 - acc: 0.9659 - val_loss: 1.6860 - val_acc: 0.6340\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0992 - acc: 0.9622 - val_loss: 1.7989 - val_acc: 0.6060\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0910 - acc: 0.9666 - val_loss: 1.6556 - val_acc: 0.6320\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0907 - acc: 0.9659 - val_loss: 1.7741 - val_acc: 0.6260\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0878 - acc: 0.9672 - val_loss: 1.6583 - val_acc: 0.6300\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.1162 - acc: 0.9548 - val_loss: 1.8872 - val_acc: 0.6220\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0802 - acc: 0.9709 - val_loss: 1.7802 - val_acc: 0.6300\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0634 - acc: 0.9796 - val_loss: 1.8276 - val_acc: 0.6260\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0685 - acc: 0.9746 - val_loss: 1.6846 - val_acc: 0.6280\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0600 - acc: 0.9796 - val_loss: 1.8089 - val_acc: 0.6300\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0567 - acc: 0.9765 - val_loss: 1.7794 - val_acc: 0.6200\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0881 - acc: 0.9734 - val_loss: 1.8684 - val_acc: 0.6320\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0830 - acc: 0.9697 - val_loss: 2.0175 - val_acc: 0.6020\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0625 - acc: 0.9783 - val_loss: 1.9768 - val_acc: 0.6260\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.0839 - acc: 0.9746 - val_loss: 1.8576 - val_acc: 0.6300\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0672 - acc: 0.9759 - val_loss: 1.9321 - val_acc: 0.6180\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0752 - acc: 0.9802 - val_loss: 1.8902 - val_acc: 0.6320\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.0813 - acc: 0.9740 - val_loss: 2.0028 - val_acc: 0.6200\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1044 - acc: 0.9635 - val_loss: 1.8667 - val_acc: 0.6500\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1099 - acc: 0.9622 - val_loss: 2.0217 - val_acc: 0.6120\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_88 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 10s 6ms/sample - loss: 2.8601 - acc: 0.2601 - val_loss: 3.0243 - val_acc: 0.2660\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 2.5375 - acc: 0.3108 - val_loss: 2.3547 - val_acc: 0.3020\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 2.4464 - acc: 0.3276 - val_loss: 2.1194 - val_acc: 0.3100\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 2.3735 - acc: 0.3288 - val_loss: 1.8496 - val_acc: 0.3440\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 2.1615 - acc: 0.3567 - val_loss: 1.7171 - val_acc: 0.3780\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.9727 - acc: 0.4037 - val_loss: 1.6303 - val_acc: 0.3880\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.9285 - acc: 0.4006 - val_loss: 1.5438 - val_acc: 0.3960\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.8096 - acc: 0.4229 - val_loss: 1.4928 - val_acc: 0.4280\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 1.7201 - acc: 0.4495 - val_loss: 1.4487 - val_acc: 0.4740\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.6975 - acc: 0.4576 - val_loss: 1.4041 - val_acc: 0.4800\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.6336 - acc: 0.4848 - val_loss: 1.3866 - val_acc: 0.4860\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.4773 - acc: 0.5102 - val_loss: 1.3569 - val_acc: 0.4900\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.4567 - acc: 0.5146 - val_loss: 1.3178 - val_acc: 0.5180\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.3537 - acc: 0.5387 - val_loss: 1.2985 - val_acc: 0.5280\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.3741 - acc: 0.5437 - val_loss: 1.3000 - val_acc: 0.5280\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.2393 - acc: 0.5777 - val_loss: 1.2826 - val_acc: 0.5160\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.2353 - acc: 0.6012 - val_loss: 1.2706 - val_acc: 0.5320\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 1.2120 - acc: 0.5895 - val_loss: 1.2572 - val_acc: 0.5400\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 1.1805 - acc: 0.6006 - val_loss: 1.2426 - val_acc: 0.5360\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.1388 - acc: 0.6074 - val_loss: 1.2351 - val_acc: 0.5480\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.0732 - acc: 0.6341 - val_loss: 1.2243 - val_acc: 0.5600\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.0663 - acc: 0.6204 - val_loss: 1.2188 - val_acc: 0.5560\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9786 - acc: 0.6545 - val_loss: 1.2122 - val_acc: 0.5700\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9992 - acc: 0.6619 - val_loss: 1.2005 - val_acc: 0.5700\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9151 - acc: 0.6681 - val_loss: 1.1954 - val_acc: 0.5660\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9139 - acc: 0.6669 - val_loss: 1.1878 - val_acc: 0.5780\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9481 - acc: 0.6601 - val_loss: 1.1786 - val_acc: 0.5740\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9123 - acc: 0.6904 - val_loss: 1.1720 - val_acc: 0.5760\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8802 - acc: 0.6916 - val_loss: 1.1646 - val_acc: 0.5720\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8306 - acc: 0.7022 - val_loss: 1.1633 - val_acc: 0.5800\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8382 - acc: 0.7040 - val_loss: 1.1718 - val_acc: 0.5760\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8140 - acc: 0.7158 - val_loss: 1.1709 - val_acc: 0.5800\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7807 - acc: 0.7133 - val_loss: 1.1583 - val_acc: 0.5800\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7357 - acc: 0.7319 - val_loss: 1.1405 - val_acc: 0.5920\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7102 - acc: 0.7523 - val_loss: 1.1403 - val_acc: 0.5900\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7319 - acc: 0.7412 - val_loss: 1.1354 - val_acc: 0.5940\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6817 - acc: 0.7529 - val_loss: 1.1499 - val_acc: 0.5840\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7227 - acc: 0.7232 - val_loss: 1.1458 - val_acc: 0.5800\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6498 - acc: 0.7542 - val_loss: 1.1451 - val_acc: 0.5940\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6753 - acc: 0.7647 - val_loss: 1.1435 - val_acc: 0.5880\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6613 - acc: 0.7684 - val_loss: 1.1311 - val_acc: 0.6000\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6667 - acc: 0.7616 - val_loss: 1.1325 - val_acc: 0.6000\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6399 - acc: 0.7622 - val_loss: 1.1224 - val_acc: 0.5900\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6066 - acc: 0.7734 - val_loss: 1.1244 - val_acc: 0.6020\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6221 - acc: 0.7666 - val_loss: 1.1298 - val_acc: 0.6040\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5420 - acc: 0.8031 - val_loss: 1.1361 - val_acc: 0.6160\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5699 - acc: 0.7833 - val_loss: 1.1277 - val_acc: 0.6180\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5586 - acc: 0.7975 - val_loss: 1.1216 - val_acc: 0.6080\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5374 - acc: 0.8000 - val_loss: 1.1148 - val_acc: 0.6180\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5077 - acc: 0.8105 - val_loss: 1.1176 - val_acc: 0.6160\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_89 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_89 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 9s 6ms/sample - loss: 2.6545 - acc: 0.2892 - val_loss: 2.8470 - val_acc: 0.3400\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.9711 - acc: 0.4142 - val_loss: 2.2016 - val_acc: 0.4220\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.6097 - acc: 0.4960 - val_loss: 1.6761 - val_acc: 0.4500\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.3592 - acc: 0.5505 - val_loss: 1.4309 - val_acc: 0.4940\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.2278 - acc: 0.6074 - val_loss: 1.3822 - val_acc: 0.5160\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9896 - acc: 0.6514 - val_loss: 1.2303 - val_acc: 0.5540\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9793 - acc: 0.6923 - val_loss: 1.2728 - val_acc: 0.5460\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.8754 - acc: 0.6966 - val_loss: 1.2165 - val_acc: 0.5620\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7626 - acc: 0.7368 - val_loss: 1.1488 - val_acc: 0.5940\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6738 - acc: 0.7560 - val_loss: 1.1084 - val_acc: 0.5960\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6973 - acc: 0.7542 - val_loss: 1.0947 - val_acc: 0.6100\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6543 - acc: 0.7684 - val_loss: 1.1180 - val_acc: 0.5980\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5384 - acc: 0.8105 - val_loss: 1.0679 - val_acc: 0.6180\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5314 - acc: 0.8087 - val_loss: 1.1061 - val_acc: 0.6120\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4867 - acc: 0.8173 - val_loss: 1.1193 - val_acc: 0.6140\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4961 - acc: 0.8192 - val_loss: 1.0944 - val_acc: 0.6140\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4038 - acc: 0.8539 - val_loss: 1.0906 - val_acc: 0.6240\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4267 - acc: 0.8464 - val_loss: 1.0908 - val_acc: 0.6420\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3867 - acc: 0.8570 - val_loss: 1.0879 - val_acc: 0.6280\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3671 - acc: 0.8675 - val_loss: 1.1407 - val_acc: 0.6280\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3646 - acc: 0.8693 - val_loss: 1.1087 - val_acc: 0.6400\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3446 - acc: 0.8811 - val_loss: 1.1140 - val_acc: 0.6260\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3084 - acc: 0.8898 - val_loss: 1.0856 - val_acc: 0.6360\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3327 - acc: 0.8811 - val_loss: 1.1468 - val_acc: 0.6240\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3273 - acc: 0.8780 - val_loss: 1.1605 - val_acc: 0.6360\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3253 - acc: 0.8762 - val_loss: 1.1216 - val_acc: 0.6380\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2989 - acc: 0.8873 - val_loss: 1.1170 - val_acc: 0.6400\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2719 - acc: 0.9096 - val_loss: 1.0992 - val_acc: 0.6560\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2855 - acc: 0.8997 - val_loss: 1.1027 - val_acc: 0.6400\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2528 - acc: 0.9040 - val_loss: 1.1362 - val_acc: 0.6380\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2712 - acc: 0.8972 - val_loss: 1.1709 - val_acc: 0.6480\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2874 - acc: 0.8972 - val_loss: 1.1157 - val_acc: 0.6520\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2513 - acc: 0.9053 - val_loss: 1.0980 - val_acc: 0.6760\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2489 - acc: 0.9096 - val_loss: 1.1364 - val_acc: 0.6560\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2544 - acc: 0.9077 - val_loss: 1.1638 - val_acc: 0.6620\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1955 - acc: 0.9307 - val_loss: 1.2101 - val_acc: 0.6540\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2334 - acc: 0.9102 - val_loss: 1.1543 - val_acc: 0.6480\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2149 - acc: 0.9263 - val_loss: 1.1750 - val_acc: 0.6580\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2421 - acc: 0.9164 - val_loss: 1.1836 - val_acc: 0.6620\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1865 - acc: 0.9337 - val_loss: 1.1597 - val_acc: 0.6520\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2217 - acc: 0.9170 - val_loss: 1.1636 - val_acc: 0.6440\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1893 - acc: 0.9313 - val_loss: 1.1896 - val_acc: 0.6400\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2057 - acc: 0.9251 - val_loss: 1.1974 - val_acc: 0.6660\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1993 - acc: 0.9300 - val_loss: 1.2026 - val_acc: 0.6780\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1954 - acc: 0.9307 - val_loss: 1.1725 - val_acc: 0.6740\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2055 - acc: 0.9257 - val_loss: 1.1785 - val_acc: 0.6660\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1907 - acc: 0.9282 - val_loss: 1.1657 - val_acc: 0.6700\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2241 - acc: 0.9139 - val_loss: 1.2628 - val_acc: 0.6600\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2039 - acc: 0.9183 - val_loss: 1.2334 - val_acc: 0.6800\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.1834 - acc: 0.9269 - val_loss: 1.2014 - val_acc: 0.6620\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_90 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 8325)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 4)                 33304     \n",
      "=================================================================\n",
      "Total params: 39,679\n",
      "Trainable params: 39,629\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 10s 6ms/sample - loss: 2.7156 - acc: 0.2941 - val_loss: 3.5960 - val_acc: 0.3700\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.7485 - acc: 0.4898 - val_loss: 1.9719 - val_acc: 0.4600\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.3127 - acc: 0.5932 - val_loss: 1.5852 - val_acc: 0.5060\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 1.0767 - acc: 0.6489 - val_loss: 1.4615 - val_acc: 0.5720\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.9506 - acc: 0.6824 - val_loss: 1.6592 - val_acc: 0.5620\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.8556 - acc: 0.7183 - val_loss: 1.4909 - val_acc: 0.5600\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.7529 - acc: 0.7511 - val_loss: 1.4179 - val_acc: 0.6000\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6918 - acc: 0.7777 - val_loss: 1.3395 - val_acc: 0.6100\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.6694 - acc: 0.7703 - val_loss: 1.3947 - val_acc: 0.6120\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5719 - acc: 0.7969 - val_loss: 1.3861 - val_acc: 0.6360\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5299 - acc: 0.8105 - val_loss: 1.3777 - val_acc: 0.6400\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.5023 - acc: 0.8341 - val_loss: 1.3690 - val_acc: 0.6440\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4681 - acc: 0.8353 - val_loss: 1.3344 - val_acc: 0.6360\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4452 - acc: 0.8390 - val_loss: 1.4241 - val_acc: 0.6420\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4329 - acc: 0.8415 - val_loss: 1.4239 - val_acc: 0.6060\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.4428 - acc: 0.8563 - val_loss: 1.3610 - val_acc: 0.6360\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3794 - acc: 0.8706 - val_loss: 1.3631 - val_acc: 0.6300\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3602 - acc: 0.8743 - val_loss: 1.4107 - val_acc: 0.6460\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3272 - acc: 0.8824 - val_loss: 1.4562 - val_acc: 0.6400\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2966 - acc: 0.8947 - val_loss: 1.4063 - val_acc: 0.6600\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3506 - acc: 0.8774 - val_loss: 1.4101 - val_acc: 0.6660\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3300 - acc: 0.8830 - val_loss: 1.4116 - val_acc: 0.6320\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3196 - acc: 0.8929 - val_loss: 1.3613 - val_acc: 0.6520\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3260 - acc: 0.8848 - val_loss: 1.5253 - val_acc: 0.6400\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2963 - acc: 0.8898 - val_loss: 1.4567 - val_acc: 0.6700\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2974 - acc: 0.8898 - val_loss: 1.4436 - val_acc: 0.6480\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.3219 - acc: 0.8972 - val_loss: 1.5621 - val_acc: 0.6380\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2843 - acc: 0.8910 - val_loss: 1.5375 - val_acc: 0.6480\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2781 - acc: 0.9071 - val_loss: 1.5188 - val_acc: 0.6660\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 8s 5ms/sample - loss: 0.2909 - acc: 0.8947 - val_loss: 1.5407 - val_acc: 0.6500\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2528 - acc: 0.9077 - val_loss: 1.5302 - val_acc: 0.6380\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2789 - acc: 0.9022 - val_loss: 1.6167 - val_acc: 0.6440\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2935 - acc: 0.8966 - val_loss: 1.6108 - val_acc: 0.6440\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2789 - acc: 0.9077 - val_loss: 1.6354 - val_acc: 0.6340\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2945 - acc: 0.8947 - val_loss: 1.6970 - val_acc: 0.6380\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2769 - acc: 0.9065 - val_loss: 1.6050 - val_acc: 0.6260\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2306 - acc: 0.9207 - val_loss: 1.6902 - val_acc: 0.6460\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2463 - acc: 0.9152 - val_loss: 1.6206 - val_acc: 0.6400\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2237 - acc: 0.9226 - val_loss: 1.6155 - val_acc: 0.6560\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2373 - acc: 0.9238 - val_loss: 1.5937 - val_acc: 0.6500\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2315 - acc: 0.9220 - val_loss: 1.5714 - val_acc: 0.6480\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 7s 4ms/sample - loss: 0.2339 - acc: 0.9232 - val_loss: 1.6766 - val_acc: 0.6520\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2592 - acc: 0.9102 - val_loss: 1.6347 - val_acc: 0.6640\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2412 - acc: 0.9164 - val_loss: 1.5003 - val_acc: 0.6720\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2834 - acc: 0.9090 - val_loss: 1.5633 - val_acc: 0.6680\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2329 - acc: 0.9201 - val_loss: 1.5663 - val_acc: 0.6600\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2106 - acc: 0.9276 - val_loss: 1.5330 - val_acc: 0.6660\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2428 - acc: 0.9146 - val_loss: 1.5558 - val_acc: 0.6500\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2692 - acc: 0.9164 - val_loss: 1.6862 - val_acc: 0.6620\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 7s 5ms/sample - loss: 0.2218 - acc: 0.9207 - val_loss: 1.6904 - val_acc: 0.6520\n",
      "Optimal learning rate: 0.0005Optimal dropout rate: 0.7\n",
      "\n",
      " Validation accuracy overall w dropout w bn: 0.662\n",
      "\n",
      " Test accuracy overall: 0.6455982\n",
      "Test accuracy person 0 0.56\n",
      "Test accuracy person 1 0.68\n",
      "Test accuracy person 2 0.5\n",
      "Test accuracy person 3 0.5\n",
      "Test accuracy person 4 0.89361703\n",
      "Test accuracy person 5 0.53061223\n",
      "Test accuracy person 6 0.86\n",
      "Test accuracy person 7 0.68\n",
      "Test accuracy person 8 0.61702126\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.2, 0.5, 0.7]\n",
    "learning_rate_list = [1e-4, 5e-4, 1e-3]\n",
    "model_list         = []\n",
    "cmax               =0 \n",
    "for dropout in dropout_rate_list:\n",
    "  for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=50,\n",
    "             validation_data=(x_valid, y_valid), verbose=False)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters = [dropout, learning_rate]\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "print (\"Optimal learning rate: \" +str(parameters[1]) +  \"Optimal dropout rate: \" +str(parameters[0]))\n",
    "\n",
    "score = model_max.evaluate(x_valid, y_valid, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Validation accuracy overall w dropout w bn:', score[1]) \n",
    "score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "for i in range(9):    \n",
    "  score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "  print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBuy_jkpb7Ve"
   },
   "source": [
    "### **Shallow CNN with dropout and with batch normalization (SCNN-dp-bn): Multiple Runs**\n",
    "\n",
    "We run SCNN-dp-bn with optimal learning rate selected in the previous block multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "_RGFUKX1TzuG",
    "outputId": "b6ed768d-7f90-4c22-d6a5-e6bdcee35e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy overall w bn w dropout: 0.6724000036716461 with sd 0.015869464094065373\n",
      "Validation accuracy persons w bn w dropout: [0.56000003 0.63283582 0.55666668 0.5962963  0.83269231 0.65849056\n",
      " 0.81555557 0.80370371 0.67000001] with sd [0.02682399 0.03350738 0.0359011  0.02844867 0.02115385 0.02727705\n",
      " 0.03449816 0.02222222 0.04494442]\n",
      "Test accuracy overall w bn w dropout: 0.6896162509918213 with sd 0.009375417878907997\n",
      "Test accuracy persons w bn w dropout: [0.592      0.678      0.584      0.65600001 0.8851064  0.60204082\n",
      " 0.84399999 0.71200001 0.66170211] with sd [0.03487119 0.02749546 0.05425865 0.03555278 0.03038905 0.06405043\n",
      " 0.0174356  0.024      0.03617021]\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.7]\n",
    "learning_rate_list = [5e-4]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "N_iter             = 10\n",
    "score_arr          = np.zeros(N_iter)\n",
    "score_person       = np.zeros((9,N_iter))\n",
    "score_arr_valid          = np.zeros(N_iter)\n",
    "score_person_valid       = np.zeros((9,N_iter))\n",
    "for n in range(N_iter):\n",
    "  for dropout in dropout_rate_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "      model.add(tf.keras.layers.Dropout(dropout))\n",
    "      model.add(tf.keras.layers.Flatten())\n",
    "      model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "      \n",
    "      optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "      model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=['accuracy'])\n",
    "  #     model.summary()\n",
    "      model.fit(x_train,\n",
    "               y_train,\n",
    "               batch_size=64,\n",
    "               epochs=50,\n",
    "               validation_data=(x_valid, y_valid), verbose=False)\n",
    "      score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "\n",
    "  score_arr_valid[n] = model.evaluate(x_valid, y_valid, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person_valid[i,n] = model.evaluate(x_valid[person_train_valid[ind_valid].T[0]==i], y_valid[person_train_valid[ind_valid].T[0]==i], verbose=0)[1]        \n",
    "\n",
    "\n",
    "  score_arr[n] = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person[i,n] = model.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)[1]    \n",
    "    \n",
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall w bn w dropout: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons w bn w dropout: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n",
    "\n",
    "score_final_overall = np.mean(score_arr)  \n",
    "score_final_std     = np.std(score_arr)\n",
    "score_final_person        = np.mean(score_person, axis=1)\n",
    "score_final_person_std    = np.std(score_person, axis=1)\n",
    "print('Test accuracy overall w bn w dropout: ' +  str(score_final_overall) + ' with sd ' + str(score_final_std))\n",
    "print('Test accuracy persons w bn w dropout: ' +  str(score_final_person)+ ' with sd ' + str(score_final_person_std)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR0N0EQVNhGQ"
   },
   "source": [
    "### **Shallow CNN with dropout and with batch normalization: Accuracy as a function of length of time segment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "Gy4nFaY5NbH1",
    "outputId": "784e3f12-9e78-4e23-c8b8-d28b0c9d85b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd395d0a0f0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropout = 0.7\n",
    "learning_rate = 0.0005\n",
    "score_time = np.zeros(4)\n",
    "time       = np.zeros(4)\n",
    "for t in range(4):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(100 + t*200,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "    time[t]   = 100 + t*200\n",
    "    x_train_t = x_train[:,0:100 + t*200,:,:]\n",
    "    x_valid_t = x_valid[:,0:100 + t*200,:,:]  \n",
    "    model.fit(x_train_t,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=50,\n",
    "             validation_data=(x_valid_t, y_valid), verbose=False)\n",
    "    score_time[t] = model.evaluate(x_valid_t, y_valid, verbose=0)[1]\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.xlabel('time')\n",
    "pyplot.plot(time,score_time)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NUZbxNINRu7"
   },
   "source": [
    "## **Deep CNN Architecture (DCNN-dp-bn)**\n",
    "**Layer 1:** 25 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout\n",
    "\n",
    "\n",
    "**Layer 2:** 50 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout\n",
    "\n",
    "**Layer 3: **100 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout\n",
    "\n",
    "**Layer 4:** 200 Conv2D filters: kernel_size = (10,1), stride = (1,1)$\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (3,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout$\\rightarrow$ Flatten\n",
    "\n",
    "**Layer 5: **FCNet with 4 outputs.\n",
    "\n",
    "We train the DCNN-dp-bn with optimal hyperparameters (learning rate, dropout) in the block below. In the next block, we fix the learned hyperparameter and run the model multiple times to find mean test accuracy and standard deviation of test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "KAiamGZzNPt0",
    "outputId": "a90b5671-0a1c-471c-f748-c8c90f57c874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal learning rate: 0.001 Optimal dropout rate: 0.5\n",
      "\n",
      " Test accuracy overall: 0.7562077\n",
      "Test accuracy person 0 0.76\n",
      "Test accuracy person 1 0.72\n",
      "Test accuracy person 2 0.66\n",
      "Test accuracy person 3 0.68\n",
      "Test accuracy person 4 0.89361703\n",
      "Test accuracy person 5 0.79591835\n",
      "Test accuracy person 6 0.74\n",
      "Test accuracy person 7 0.74\n",
      "Test accuracy person 8 0.82978725\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.2, 0.5, 0.7]\n",
    "learning_rate_list = [1e-4, 5e-4, 1e-3]\n",
    "# dropout_rate_list  = [0.5]\n",
    "# learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               =0 \n",
    "for dropout in dropout_rate_list:\n",
    "  for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))          \n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))  \n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    # model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    # model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    #from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    #checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=100,\n",
    "             validation_data=(x_valid, y_valid), verbose=False)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters = [dropout, learning_rate]\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "print (\"Optimal learning rate: \" +str(parameters[1]) +  \" Optimal dropout rate: \" +str(parameters[0]))\n",
    "score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "for i in range(9):    \n",
    "  score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "  print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1QWj8dTeZEf"
   },
   "source": [
    "### **Deep CNN with dropout and with batch normalization (DCNN-dp-bn): Multiple Runs**\n",
    "\n",
    "We run DCNN-dp-bn with optimal learning rate selected in the previous block multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "mTcsuVX2Ua1p",
    "outputId": "135a9d9c-48ff-40fc-f9d2-1e0f639ff2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Validation accuracy overall w bn w dropout: 0.7592000126838684 with sd 0.01656984946202298\n",
      "Validation accuracy persons w bn w dropout: [0.64897959 0.75102041 0.64000002 0.6030303  0.88936172 0.653125\n",
      " 0.90303031 0.93684212 0.80350878] with sd [0.05384043 0.05536597 0.01662959 0.02606765 0.02481256 0.03186887\n",
      " 0.03119887 0.0140351  0.0301836 ]\n",
      "Test accuracy overall w bn w dropout: 0.7444695353507995 with sd 0.013797484415835571\n",
      "Test accuracy persons w bn w dropout: [0.71999999 0.73200001 0.612      0.65600001 0.89361703 0.71836735\n",
      " 0.792      0.79999999 0.78723406] with sd [0.04195234 0.04664761 0.02993325 0.04630335 0.0134565  0.02707449\n",
      " 0.00979798 0.0219089  0.03560257]\n"
     ]
    }
   ],
   "source": [
    "dropout_rate_list  = [0.5]\n",
    "learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "N_iter             = 5\n",
    "score_arr          = np.zeros(N_iter)\n",
    "score_person       = np.zeros((9,N_iter))\n",
    "score_arr_valid          = np.zeros(N_iter)\n",
    "score_person_valid       = np.zeros((9,N_iter))\n",
    "for n in range(N_iter):\n",
    "  for dropout in dropout_rate_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Must define the input shape in the first layer of the neural network\n",
    "      model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "      model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "      model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "      model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "      model.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "      model.add(tf.keras.layers.Dropout(dropout))          \n",
    "\n",
    "      model.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "      model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "      model.add(tf.keras.layers.Dropout(dropout))  \n",
    "\n",
    "      model.add(tf.keras.layers.Flatten())\n",
    "      # model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "      # model.add(tf.keras.layers.Dropout(0.5))\n",
    "      model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "      \n",
    "      optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "      model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=['accuracy'])\n",
    "  #     model.summary()\n",
    "      model.fit(x_train,\n",
    "               y_train,\n",
    "               batch_size=64,\n",
    "               epochs=100,\n",
    "               validation_data=(x_valid, y_valid), verbose=False)\n",
    "      score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "\n",
    "  score_arr_valid[n] = model.evaluate(x_valid, y_valid, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person_valid[i,n] = model.evaluate(x_valid[person_train_valid[ind_valid].T[0]==i], y_valid[person_train_valid[ind_valid].T[0]==i], verbose=0)[1]        \n",
    "\n",
    "\n",
    "  score_arr[n] = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "\n",
    "  for i in range(9):    \n",
    "    score_person[i,n] = model.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)[1]    \n",
    "    \n",
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall w bn w dropout: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons w bn w dropout: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n",
    "\n",
    "score_final_overall = np.mean(score_arr)  \n",
    "score_final_std     = np.std(score_arr)\n",
    "score_final_person        = np.mean(score_person, axis=1)\n",
    "score_final_person_std    = np.std(score_person, axis=1)\n",
    "print('Test accuracy overall w bn w dropout: ' +  str(score_final_overall) + ' with sd ' + str(score_final_std))\n",
    "print('Test accuracy persons w bn w dropout: ' +  str(score_final_person)+ ' with sd ' + str(score_final_person_std)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06-F-0NgwTXf"
   },
   "source": [
    "#**RNN Architectures for Motor Imagery Classification**\n",
    "\n",
    "In this section, we train models based on RNN for motor imagery classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qtnOV5igCH6"
   },
   "source": [
    "### **Data Reloading** \n",
    "\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_P69MZgfd_G"
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"path_name")\n",
    "y_test = np.load(\"path_name")\n",
    "person_train_valid = np.load(\"path_name")\n",
    "X_train_valid = np.load(\"path_name")\n",
    "y_train_valid = np.load(\"path_name")\n",
    "person_test = np.load(\"path_name")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gN__CR2gapc"
   },
   "source": [
    "### **Random Splitting and shaping of the Data**\n",
    "\n",
    "\n",
    "Splitting of the X_train_valid_data. We keep 500 data points out of 2115 for validation.\n",
    "\n",
    "\n",
    "We also make the labels one hot encoded for inputting into cross entropy function for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXubrQm4zbqW"
   },
   "outputs": [],
   "source": [
    "ind_valid = np.random.choice(2115, 500, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "(x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 4)\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "\n",
    "\n",
    "x_test  = X_test\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l-xw7DoldgK"
   },
   "source": [
    "## **LSTM**\n",
    "\n",
    "Layer 1: LSTM$\\rightarrow$ tanh \n",
    "\n",
    "Layer 2: FCNet with 4 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "UDjuQ4l_0Ij_",
    "outputId": "5dcafcfb-09a7-4ddd-d267-f4f216e1f032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                3680      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 3,764\n",
      "Trainable params: 3,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.4812 - acc: 0.2223 - val_loss: 1.4501 - val_acc: 0.2820\n",
      "Epoch 2/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.4258 - acc: 0.2508 - val_loss: 1.4359 - val_acc: 0.2760\n",
      "Epoch 3/20\n",
      "1615/1615 [==============================] - 17s 11ms/sample - loss: 1.4001 - acc: 0.2737 - val_loss: 1.4314 - val_acc: 0.2740\n",
      "Epoch 4/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3855 - acc: 0.2879 - val_loss: 1.4352 - val_acc: 0.2380\n",
      "Epoch 5/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3763 - acc: 0.3009 - val_loss: 1.4300 - val_acc: 0.2480\n",
      "Epoch 6/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3631 - acc: 0.3071 - val_loss: 1.4309 - val_acc: 0.2240\n",
      "Epoch 7/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3520 - acc: 0.3195 - val_loss: 1.4286 - val_acc: 0.2380\n",
      "Epoch 8/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3430 - acc: 0.3282 - val_loss: 1.4259 - val_acc: 0.2320\n",
      "Epoch 9/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3363 - acc: 0.3300 - val_loss: 1.4279 - val_acc: 0.2420\n",
      "Epoch 10/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3301 - acc: 0.3443 - val_loss: 1.4257 - val_acc: 0.2580\n",
      "Epoch 11/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3209 - acc: 0.3622 - val_loss: 1.4295 - val_acc: 0.2520\n",
      "Epoch 12/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3145 - acc: 0.3808 - val_loss: 1.4269 - val_acc: 0.2540\n",
      "Epoch 13/20\n",
      "1615/1615 [==============================] - 17s 11ms/sample - loss: 1.3085 - acc: 0.3820 - val_loss: 1.4278 - val_acc: 0.2520\n",
      "Epoch 14/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.3029 - acc: 0.3895 - val_loss: 1.4281 - val_acc: 0.2480\n",
      "Epoch 15/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2955 - acc: 0.3938 - val_loss: 1.4287 - val_acc: 0.2400\n",
      "Epoch 16/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2897 - acc: 0.4019 - val_loss: 1.4297 - val_acc: 0.2400\n",
      "Epoch 17/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2838 - acc: 0.4000 - val_loss: 1.4349 - val_acc: 0.2520\n",
      "Epoch 18/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2807 - acc: 0.4124 - val_loss: 1.4355 - val_acc: 0.2500\n",
      "Epoch 19/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2751 - acc: 0.4173 - val_loss: 1.4367 - val_acc: 0.2520\n",
      "Epoch 20/20\n",
      "1615/1615 [==============================] - 18s 11ms/sample - loss: 1.2675 - acc: 0.4229 - val_loss: 1.4369 - val_acc: 0.2340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe5baaf70f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(20, input_shape = (1000,25), return_sequences=False))\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    #from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    #checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=20,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VbOKqtbn_Fw"
   },
   "source": [
    "## **LSTM-crop**\n",
    "Layer 1: LSTM$\\rightarrow$ tanh $\\rightarrow $ Dropout\n",
    "\n",
    "Layer 2: FCNet with 4 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oKfIKhdlFYK"
   },
   "source": [
    "### **Construct continguous subsequences **\n",
    "Since RNN does not train. We suspect that the length of the sequence is a problem. Therefore, we construct contiguous subsequences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91KCHLASXbLX"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Crop the data into contiguous subsequences\n",
    "X_train_valid_temp = np.array(np.swapaxes(X_train_valid, 1,2))\n",
    "X_test_temp = np.array(np.swapaxes(X_test, 1,2))\n",
    "len_crop = 50   ## Length of the continguous subsequence\n",
    "n_total_crop  = X_train_valid_temp.shape[1] - len_crop + 1\n",
    "\n",
    "n_crop = 100    ## No. of subsequences for each subject\n",
    "n_t = X_train_valid_temp.shape[0]\n",
    "n  = X_train_valid_temp.shape[1]\n",
    "n_t1 = X_test_temp.shape[0]\n",
    "n1  = X_test_temp.shape[1]\n",
    "n_crop_div  = n//len_crop\n",
    "n_crop_div1  = n1//len_crop\n",
    "\n",
    "n_electrodes = X_train_valid_temp.shape[2]\n",
    "m = n_t*n_crop\n",
    "X_train_valid_crop = np.zeros((m,len_crop,n_electrodes))\n",
    "y_train_valid_crop = np.zeros(m)\n",
    "person_train_valid_crop = np.zeros(m)\n",
    "X_test_crop = np.zeros((m,len_crop,n_electrodes))\n",
    "y_test_crop = np.zeros(m)\n",
    "\n",
    "k=0\n",
    "\n",
    "a=0\n",
    "for i in range(n_t):\n",
    "    for j in range(n_crop):\n",
    "        k = np.random.randint(n_crop_div-1)\n",
    "\n",
    "        s = np.random.randint(len_crop)\n",
    "#        print (k)\n",
    "        X_train_valid_crop[a,:,:] = X_train_valid_temp[i,k*len_crop+s: (k+1)*len_crop+s, :]\n",
    "        y_train_valid_crop[a] = y_train_valid[i]\n",
    "        person_train_valid_crop[a] = person_train_valid[i]\n",
    "        a = a+1\n",
    "a=0\n",
    "\n",
    "        \n",
    "ind_subset = np.random.choice(211500,100000, replace=False)\n",
    "ind_subset1 = np.random.choice(44300,10000, replace=False)\n",
    "X_train_valid_crop            = X_train_valid_crop[ind_subset,:,:]\n",
    "y_train_valid_crop            = y_train_valid_crop[ind_subset]      \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5446miLpWOC"
   },
   "source": [
    "### **Random Splitting of the cropped data and creating one hot encoded labels for the cropped data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eretE7sQXfA4"
   },
   "outputs": [],
   "source": [
    "ind_s = np.random.choice(100000,10000, replace=False)\n",
    "ind_s_r = np.array(list(set(range(100000)).difference(set(ind_s))))\n",
    "(x_train_c, x_valid_c) = X_train_valid_crop[ind_s_r], X_train_valid_crop[ind_s] \n",
    "(y_train_c, y_valid_c) = y_train_valid_crop[ind_s_r], y_train_valid_crop[ind_s]\n",
    "(person_train_c, person_valid_c) = person_train_valid_crop[ind_s_r], person_train_valid_crop[ind_s]\n",
    "y_train_c = tf.keras.utils.to_categorical(y_train_c, 4)\n",
    "y_valid_c = tf.keras.utils.to_categorical(y_valid_c, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1683
    },
    "colab_type": "code",
    "id": "sTqmyICCZQNI",
    "outputId": "7e84b2b6-740b-4943-96d8-cac131f4eab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                15200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 15,604\n",
      "Trainable params: 15,504\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "90000/90000 [==============================] - 87s 969us/sample - loss: 1.4474 - acc: 0.2565 - val_loss: 1.3864 - val_acc: 0.2629\n",
      "Epoch 2/50\n",
      "90000/90000 [==============================] - 85s 949us/sample - loss: 1.3872 - acc: 0.2590 - val_loss: 1.3834 - val_acc: 0.2648\n",
      "Epoch 3/50\n",
      "90000/90000 [==============================] - 86s 953us/sample - loss: 1.3858 - acc: 0.2644 - val_loss: 1.3810 - val_acc: 0.2727\n",
      "Epoch 4/50\n",
      "90000/90000 [==============================] - 86s 958us/sample - loss: 1.3848 - acc: 0.2662 - val_loss: 1.3784 - val_acc: 0.2768\n",
      "Epoch 5/50\n",
      "90000/90000 [==============================] - 86s 960us/sample - loss: 1.3828 - acc: 0.2748 - val_loss: 1.3764 - val_acc: 0.2855\n",
      "Epoch 6/50\n",
      "90000/90000 [==============================] - 87s 962us/sample - loss: 1.3808 - acc: 0.2782 - val_loss: 1.3718 - val_acc: 0.2909\n",
      "Epoch 7/50\n",
      "90000/90000 [==============================] - 87s 969us/sample - loss: 1.3805 - acc: 0.2801 - val_loss: 1.3694 - val_acc: 0.2951\n",
      "Epoch 8/50\n",
      "90000/90000 [==============================] - 88s 980us/sample - loss: 1.3792 - acc: 0.2843 - val_loss: 1.3691 - val_acc: 0.2963\n",
      "Epoch 9/50\n",
      "90000/90000 [==============================] - 88s 981us/sample - loss: 1.3784 - acc: 0.2831 - val_loss: 1.3672 - val_acc: 0.3046\n",
      "Epoch 10/50\n",
      "90000/90000 [==============================] - 88s 976us/sample - loss: 1.3778 - acc: 0.2851 - val_loss: 1.3684 - val_acc: 0.2942\n",
      "Epoch 11/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3781 - acc: 0.2861 - val_loss: 1.3662 - val_acc: 0.3104\n",
      "Epoch 12/50\n",
      "90000/90000 [==============================] - 88s 972us/sample - loss: 1.3769 - acc: 0.2886 - val_loss: 1.3659 - val_acc: 0.3017\n",
      "Epoch 13/50\n",
      "90000/90000 [==============================] - 88s 979us/sample - loss: 1.3758 - acc: 0.2911 - val_loss: 1.3652 - val_acc: 0.3029\n",
      "Epoch 14/50\n",
      "90000/90000 [==============================] - 88s 976us/sample - loss: 1.3760 - acc: 0.2900 - val_loss: 1.3632 - val_acc: 0.3111\n",
      "Epoch 15/50\n",
      "90000/90000 [==============================] - 87s 972us/sample - loss: 1.3761 - acc: 0.2888 - val_loss: 1.3651 - val_acc: 0.3081\n",
      "Epoch 16/50\n",
      "90000/90000 [==============================] - 88s 976us/sample - loss: 1.3757 - acc: 0.2899 - val_loss: 1.3637 - val_acc: 0.3090\n",
      "Epoch 17/50\n",
      "90000/90000 [==============================] - 88s 975us/sample - loss: 1.3752 - acc: 0.2919 - val_loss: 1.3627 - val_acc: 0.3089\n",
      "Epoch 18/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3743 - acc: 0.2936 - val_loss: 1.3648 - val_acc: 0.3105\n",
      "Epoch 19/50\n",
      "90000/90000 [==============================] - 87s 972us/sample - loss: 1.3739 - acc: 0.2955 - val_loss: 1.3604 - val_acc: 0.3138\n",
      "Epoch 20/50\n",
      "90000/90000 [==============================] - 88s 976us/sample - loss: 1.3740 - acc: 0.2932 - val_loss: 1.3629 - val_acc: 0.3116\n",
      "Epoch 21/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3736 - acc: 0.2950 - val_loss: 1.3610 - val_acc: 0.3135\n",
      "Epoch 22/50\n",
      "90000/90000 [==============================] - 88s 975us/sample - loss: 1.3740 - acc: 0.2909 - val_loss: 1.3617 - val_acc: 0.3102\n",
      "Epoch 23/50\n",
      "90000/90000 [==============================] - 88s 975us/sample - loss: 1.3733 - acc: 0.2940 - val_loss: 1.3602 - val_acc: 0.3151\n",
      "Epoch 24/50\n",
      "90000/90000 [==============================] - 88s 975us/sample - loss: 1.3725 - acc: 0.2969 - val_loss: 1.3598 - val_acc: 0.3127\n",
      "Epoch 25/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3740 - acc: 0.2947 - val_loss: 1.3591 - val_acc: 0.3193\n",
      "Epoch 26/50\n",
      "90000/90000 [==============================] - 88s 973us/sample - loss: 1.3736 - acc: 0.2942 - val_loss: 1.3613 - val_acc: 0.3172\n",
      "Epoch 27/50\n",
      "90000/90000 [==============================] - 88s 975us/sample - loss: 1.3729 - acc: 0.2978 - val_loss: 1.3624 - val_acc: 0.3062\n",
      "Epoch 28/50\n",
      "90000/90000 [==============================] - 87s 972us/sample - loss: 1.3736 - acc: 0.2937 - val_loss: 1.3604 - val_acc: 0.3158\n",
      "Epoch 29/50\n",
      "90000/90000 [==============================] - 87s 970us/sample - loss: 1.3733 - acc: 0.2937 - val_loss: 1.3602 - val_acc: 0.3130\n",
      "Epoch 30/50\n",
      "90000/90000 [==============================] - 87s 968us/sample - loss: 1.3725 - acc: 0.2952 - val_loss: 1.3599 - val_acc: 0.3121\n",
      "Epoch 31/50\n",
      "90000/90000 [==============================] - 88s 973us/sample - loss: 1.3714 - acc: 0.2986 - val_loss: 1.3586 - val_acc: 0.3143\n",
      "Epoch 32/50\n",
      "90000/90000 [==============================] - 87s 969us/sample - loss: 1.3723 - acc: 0.2952 - val_loss: 1.3583 - val_acc: 0.3106\n",
      "Epoch 33/50\n",
      "90000/90000 [==============================] - 87s 969us/sample - loss: 1.3722 - acc: 0.2984 - val_loss: 1.3586 - val_acc: 0.3206\n",
      "Epoch 34/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3723 - acc: 0.2953 - val_loss: 1.3585 - val_acc: 0.3122\n",
      "Epoch 35/50\n",
      "90000/90000 [==============================] - 87s 970us/sample - loss: 1.3720 - acc: 0.2959 - val_loss: 1.3583 - val_acc: 0.3212\n",
      "Epoch 36/50\n",
      "90000/90000 [==============================] - 87s 969us/sample - loss: 1.3712 - acc: 0.2970 - val_loss: 1.3581 - val_acc: 0.3161\n",
      "Epoch 37/50\n",
      "90000/90000 [==============================] - 87s 970us/sample - loss: 1.3709 - acc: 0.3002 - val_loss: 1.3562 - val_acc: 0.3183\n",
      "Epoch 38/50\n",
      "90000/90000 [==============================] - 88s 974us/sample - loss: 1.3711 - acc: 0.3004 - val_loss: 1.3577 - val_acc: 0.3167\n",
      "Epoch 39/50\n",
      "90000/90000 [==============================] - 87s 971us/sample - loss: 1.3713 - acc: 0.2985 - val_loss: 1.3570 - val_acc: 0.3200\n",
      "Epoch 40/50\n",
      "90000/90000 [==============================] - 87s 971us/sample - loss: 1.3712 - acc: 0.3016 - val_loss: 1.3571 - val_acc: 0.3168\n",
      "Epoch 41/50\n",
      "90000/90000 [==============================] - 88s 973us/sample - loss: 1.3712 - acc: 0.2999 - val_loss: 1.3565 - val_acc: 0.3178\n",
      "Epoch 42/50\n",
      "13824/90000 [===>..........................] - ETA: 1:12 - loss: 1.3732 - acc: 0.2978"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(50, dropout=0.5, recurrent_dropout=0.5,input_shape = (50,25), return_sequences=False))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_c,\n",
    "         y_train_c,\n",
    "         batch_size=64,\n",
    "         epochs=50,\n",
    "         validation_data=(x_valid_c, y_valid_c), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCd8o5tBq1oJ"
   },
   "source": [
    "### **LSTM-crop : Multiple Runs**\n",
    "\n",
    "We run LSTM-crop multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UUKtRe0XAs2"
   },
   "outputs": [],
   "source": [
    "dropout_rate_list  = [0.5]\n",
    "learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "N_iter             = 10\n",
    "score_arr          = np.zeros(N_iter)\n",
    "score_person       = np.zeros((9,N_iter))\n",
    "score_arr_valid          = np.zeros(N_iter)\n",
    "score_person_valid       = np.zeros((9,N_iter))\n",
    "for n in range(N_iter):\n",
    "  for dropout in dropout_rate_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(tf.keras.layers.LSTM(50, dropout=0.5, recurrent_dropout=0.5,input_shape = (50,25), return_sequences=False))\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "      model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "      model.summary()\n",
    "\n",
    "      model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "      model.fit(x_train_c,\n",
    "               y_train_c,\n",
    "               batch_size=64,\n",
    "               epochs=2,\n",
    "               validation_data=(x_valid_c, y_valid_c), verbose=True)\n",
    "      score = model.evaluate(x_valid_c, y_valid_c, verbose=0)\n",
    "\n",
    "  score_arr_valid[n] = model.evaluate(x_train_c, y_train_c, verbose=0)[1]\n",
    "\n",
    "    \n",
    "score_final_overall_valid = np.mean(score_arr_valid)  \n",
    "score_final_std_valid     = np.std(score_arr_valid)\n",
    "score_final_person_valid        = np.mean(score_person_valid, axis=1)\n",
    "score_final_person_std_valid    = np.std(score_person_valid, axis=1)\n",
    "print('Validation accuracy overall: ' +  str(score_final_overall_valid) + ' with sd ' + str(score_final_std_valid))\n",
    "print('Validation accuracy persons: ' +  str(score_final_person_valid)+ ' with sd ' + str(score_final_person_std_valid)) \n",
    "\n",
    "# score_final_overall = np.mean(score_arr)  \n",
    "# score_final_std     = np.std(score_arr)\n",
    "# score_final_person        = np.mean(score_person, axis=1)\n",
    "# score_final_person_std    = np.std(score_person, axis=1)\n",
    "# print('Test accuracy overall w bn w dropout: ' +  str(score_final_overall) + ' with sd ' + str(score_final_std))\n",
    "# print('Test accuracy persons w bn w dropout: ' +  str(score_final_person)+ ' with sd ' + str(score_final_person_std)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ro3Wx0mrrJ3o"
   },
   "source": [
    "#**CNN-->RNN Architectures for Motor Imagery Classification**\n",
    "\n",
    "In this section, we train models based on RNN for motor imagery classification. Since RNN with LSTM alone does not work. We tried an architecture which combines CNN with RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdOk-1PVNH7G"
   },
   "source": [
    "### ** Data Loading**\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZkNqqwdr46j"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_test.npy\")\n",
    "y_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_test.npy\")\n",
    "person_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_train_valid.npy\")\n",
    "person_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_test.npy\")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_40f91D6svHN"
   },
   "source": [
    "## **Random Splitting and shaping of the Data**\n",
    "\n",
    "\n",
    "Splitting of the X_train_valid_data. We keep 500 data points out of 2115 for validation.\n",
    "\n",
    "We also reshape the data into a 4 dimensional tensor for inputting into CNN later.\n",
    "\n",
    "We also make the labels one hot encoded for inputting into cross entropy function for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YPYuNQ61ZQNL",
    "outputId": "0a1ff386-0f0f-4fd1-85a4-9c9a787cb8cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1615, 4)\n"
     ]
    }
   ],
   "source": [
    "ind_valid = np.random.choice(2115, 500, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "(x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "# Reshape input data from (25,1000) to (25,1000, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 4)\n",
    "\n",
    "x_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaqsI1Ew4Y0p"
   },
   "source": [
    "## **CNN-LSTM **\n",
    "\n",
    "**Layer 1:** 25 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (1,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout\n",
    "\n",
    "\n",
    "**Layer 2: **50 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (1,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout\n",
    "\n",
    "**Layer 3:** 50 Conv2D filters: kernel_size = (10,1), stride = (1,1) $\\rightarrow$ ELU $\\rightarrow$ MaxPool: size=(3,1), stride = (1,1) $\\rightarrow$ BNorm $\\rightarrow$ Dropout$\\rightarrow$ Flatten\n",
    "\n",
    "**Layer 4: **FCNet with 50 outputs \n",
    "\n",
    "**Layer 5:** LSTM with 20 outputs $\\rightarrow$ tanh\n",
    "\n",
    "**Layer 6: **FCNet with 4 outputs.\n",
    "\n",
    "We train the CNN-LSTM with optimal hyperparameters (learning rate, dropout) in the block below. In the second block, we fix the learned hyperparameter and run the model multiple times to find mean test accuracy and standard deviation of test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-mESmEx0KPL"
   },
   "outputs": [],
   "source": [
    "\n",
    "dropout_rate_list  = [0.2, 0.5, 0.7]\n",
    "learning_rate_list = [1e-4, 5e-4, 1e-3]\n",
    "dropout_rate_list  = [0.5]\n",
    "learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               = 0 \n",
    "for dropout in dropout_rate_list:\n",
    "  for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense((50)))\n",
    "    model.add(tf.keras.layers.Reshape((50,1)))\n",
    "    model.add(tf.keras.layers.LSTM(20, dropout=0.5, recurrent_dropout=0.5, input_shape=(50,1), return_sequences=False))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=5,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters = [dropout, learning_rate]\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "print (\"Optimal learning rate: \" +str(parameters[1]) +  \" Optimal dropout rate: \" +str(parameters[0]))\n",
    "\n",
    "score = model_max.evaluate(x_valid, y_valid, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Validation accuracy overall w/o dropout bn:', score[1]) \n",
    "score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "for i in range(9):    \n",
    "  score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "  print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dAAVFwI7Szx"
   },
   "source": [
    "## **CNN-LSTM: Multiple Runs**\n",
    "We run CNN-LSTM with optimal learning rate selected in the previous block multiple times to find the mean test accuracy and standard deviation of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs7eaeOorsSC"
   },
   "outputs": [],
   "source": [
    "dropout            = 0.5\n",
    "learning_rate      = 1e-3\n",
    "iteration          = 10\n",
    "score_train_1      = np.zeros((iteration,))\n",
    "score_valid_1      = np.zeros((iteration,))\n",
    "score_test_1       = np.zeros((iteration,))\n",
    "#history_dict = {}\n",
    "score_person       = np.zeros((9, iteration))\n",
    "\n",
    "for i in range(iteration):\n",
    "    print ('iteration: ' + str(i))\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    #model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,25), padding='same', activation=''))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense((50)))\n",
    "    model.add(tf.keras.layers.Reshape((50,1)))\n",
    "    model.add(tf.keras.layers.LSTM(20, dropout=0.5, recurrent_dropout=0.5, input_shape=(50,1), return_sequences=False))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    #model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    #earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  #min_delta=0,\n",
    "                                  #patience=25,\n",
    "                                  #verbose=0, mode='auto')\n",
    "    #callback = [earlystop]\n",
    "    \n",
    "    history = model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size= 64,\n",
    "         epochs=10, validation_data=(x_valid, y_valid), verbose = 0 )\n",
    "                        #callbacks = callback, verbose = 0)\n",
    "    \n",
    "    score1 = model.evaluate(x_train, y_train)\n",
    "    score_train_1[i] = score1[1]\n",
    "    score2 = model.evaluate(x_valid, y_valid)\n",
    "    score_valid_1[i] = score2[1]\n",
    "    score3 = model.evaluate(x_test, y_test)\n",
    "    score_test_1[i] = score3[1]\n",
    "    \n",
    "    for j in range(9):    \n",
    "        score_person[j,i] = model.evaluate(x_test[person_test.T[0]==j], y_test[person_test.T[0]==j], verbose=0)[1]  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sCI1ZbcbPnJ"
   },
   "source": [
    "# **VAE based Interpretation for Motor Imagery Classification**\n",
    "\n",
    "In this section, we interpret one of the black box models we implemented. We use the best performing model DCNN-dp-bn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOzSi8QPCaLx"
   },
   "source": [
    "### **Data Reloading** \n",
    "\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0ebQWsoB5OL"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_test.npy\")\n",
    "y_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_test.npy\")\n",
    "person_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_train_valid.npy\")\n",
    "person_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_test.npy\")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-yWKBGUCzG2"
   },
   "source": [
    "### **Random Splitting and shaping of the Data**\n",
    "\n",
    "\n",
    "Splitting of the X_train_valid_data. We keep 500 data points out of 2115 for validation.\n",
    "\n",
    "We also reshape the data into a 4 dimensional tensor for inputting into CNN later.\n",
    "\n",
    "We also make the labels one hot encoded for inputting into cross entropy function for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "VPp79pelC0RU",
    "outputId": "320eb084-53d9-45d1-c738-1a1b61a6bce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1615, 1000, 1, 25)\n",
      "(1615, 4)\n",
      "(443, 1000, 1, 25)\n",
      "(443, 4)\n",
      "(500, 1000, 1, 25)\n",
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "# random splitting\n",
    "ind_valid = np.random.choice(2115, 500, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "(x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 4)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "\n",
    "x_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_test.shape)\n",
    "print (x_valid.shape)\n",
    "print (y_valid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GIxqwTuDAIT"
   },
   "source": [
    "## **DCNN-dp-bn**\n",
    "\n",
    "Train DCNN-dp-bn as it is the best performing architecture. We will interpret this architecture in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJievvd4bZZs"
   },
   "outputs": [],
   "source": [
    "\n",
    "dropout_rate_list  = [0.5]\n",
    "learning_rate_list = [1e-3]\n",
    "model_list         = []\n",
    "cmax               =0 \n",
    "for dropout in dropout_rate_list:\n",
    "  for learning_rate in learning_rate_list:\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "    model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))          \n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))  \n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    # model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    # model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    #from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    #checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=100,\n",
    "             validation_data=(x_valid, y_valid), verbose=False)\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    if(score[1]>cmax):\n",
    "      cmax = score[1]\n",
    "      parameters = [dropout, learning_rate]\n",
    "      model_max  = model\n",
    "    model_list.append(model)\n",
    "\n",
    "# print (\"Optimal learning rate: \" +str(parameters[1]) +  \" Optimal dropout rate: \" +str(parameters[0]))\n",
    "# score = model_max.evaluate(x_test, y_test, verbose=0)\n",
    "# # Print test accuracy\n",
    "# print('\\n', 'Test accuracy overall:', score[1])\n",
    "\n",
    "# for i in range(9):    \n",
    "#   score_person = model_max.evaluate(x_test[person_test.T[0]==i], y_test[person_test.T[0]==i], verbose=0)    \n",
    "#   print(\"Test accuracy person \" + str(i) +  \" \" + str(score_person[1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "vnhJ7g7dHN9Z",
    "outputId": "4004b09c-46b7-4f3d-ba6c-c73b558e86b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 1000, 1, 25)       6275      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 333, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 333, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 333, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 111, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 111, 1, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 111, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 111, 1, 100)       50100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 37, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 37, 1, 100)        400       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 37, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 37, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 12, 1, 200)        800       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 9604      \n",
      "=================================================================\n",
      "Total params: 280,229\n",
      "Trainable params: 279,479\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uiE0VZhaeNf"
   },
   "source": [
    "## Extract Features From DCNN-dp-bn\n",
    "\n",
    "We extract the hidden layer values for each raw data point. We pass the data into the model and extract the output from the last convolutional layer after it is flattened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCyFX0o9cDjz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "layer_name = 'flatten_10' ### Please use the correct name of the layer or else an error shows up. To do so print (model.summary() and see the name of the flatten layer)\n",
    "## We use the model that we just trained in the block above to extract the features.\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "x_train_new = intermediate_layer_model.predict(x_train) ## Feature extracted from the hidden layer\n",
    "x_valid_new = intermediate_layer_model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfgxvr11G5Ml"
   },
   "source": [
    "## **Variational AutoEncoder (VAE)**\n",
    "\n",
    "We train a VAE below to extract a lower dimensional representation of the hidden features extracted from the DCNN-dp-bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzTwzOvBG4Tg"
   },
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "#   with tf.variable_scope(\"encoder\", reuse=False):\n",
    "    x = tf.layers.dense(inputs=x, units= 500, activation='elu')            \n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.layers.dense(x, units= 100, activation='elu')            \n",
    "    x = tf.layers.batch_normalization(x)  \n",
    "    mean  = tf.layers.dense(x, units=2)\n",
    "    std   = 0.5 * tf.layers.dense(x, units=2)            \n",
    "    epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], 2])) \n",
    "    z  = mean + tf.multiply(epsilon, tf.exp(std))            \n",
    "    return mean, std,z\n",
    "\n",
    "def decoder(z):\n",
    "#   with tf.variable_scope(\"decoder\", reuse=False):  \n",
    "    x = tf.layers.dense(z, units= 100, activation='elu')            \n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.layers.dense(x, units= 500, activation='elu')            \n",
    "    x = tf.layers.batch_normalization(x)  \n",
    "    x = tf.layers.dense(x, units= 2400)  \n",
    "    return x\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2400))\n",
    "z_mean, z_std, z = encoder(x)\n",
    "x_reconstruct = decoder(z)  \n",
    "recons_loss = tf.reduce_sum(tf.squared_difference(x_reconstruct, x))\n",
    "KL_div_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_std - tf.square(z_mean) - tf.exp(2.0 * z_std), 1)\n",
    "loss =tf.reduce_mean(KL_div_loss + recons_loss)\n",
    "optim = tf.train.AdamOptimizer().minimize(loss)\n",
    "num_epoch = 100\n",
    "data_len  = 1615\n",
    "batch_size = 64\n",
    "data= x_train_new\n",
    "z_train_new = np.zeros((data_len,2))\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for epoch in range(num_epoch):\n",
    "       batch_idxs = data_len//batch_size\n",
    "       for b_idx in range(batch_idxs):\n",
    "          idx  =  np.random.choice(data_len,batch_size, replace=True)\n",
    "          batch = data[idx]\n",
    "          sess.run(optim, feed_dict = {x: batch})\n",
    "  for t in range(10):        \n",
    "    z_train_new += sess.run(z, feed_dict = {x:data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGd2AFC9KSKz"
   },
   "outputs": [],
   "source": [
    "z_train_new = z_train_new/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Gs3cNumJqTH"
   },
   "source": [
    "## **Plot the lower dimensional representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYtyv_KWJl5k"
   },
   "outputs": [],
   "source": [
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        j = X.shape[0]-i-1\n",
    "        plt.text(X[j, 0], X[j, 1], str(y[j]),\n",
    "                 color=plt.cm.Set1((y[j]+1) / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "#     plt.savefig('/content/gdrive/My Drive/ECE_239_AS/Project_final/project/interpret_vae.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "sIbBNufeI6Cv",
    "outputId": "5c48a8f3-47a6-4ab1-f910-ce08cc1953f8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAFKCAYAAABCYmUbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAiniIioIAiCIJwiIqKCIAiCcIr8fzL7CXNWx3SsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = z_train_new\n",
    "y = y_train_valid[ind_train]\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "\n",
    "plot_embedding(X,\"VAE epresentation from the model (time %.2fs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3StzGLOuSiQ"
   },
   "source": [
    "## **Isomap** \n",
    "\n",
    "We also looked at the representations computed by using isomaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAycCQYAO4aC"
   },
   "source": [
    "### **DCNN-dp-bn Isomap**\n",
    "Below we compute the Isomap of the hidden features extracted by the DCNN-dp-bn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "hGny8mXtP0kU",
    "outputId": "89dbd673-5794-41af-9b65-42910a2e73e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Isomap embedding\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAFKCAYAAABCYmUbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = x_train_new\n",
    "y = y_train_valid[ind_train]\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "plot_embedding(X_iso,\n",
    "               \"Isomap projection of representation from the model (time %.2fs)\" %\n",
    "               (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mku7krXguaq9"
   },
   "source": [
    "### **Model evaluation on 4 classes and compare to VAE and Isomap plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "n5O2PP8lgFU7",
    "outputId": "84dd2730-ad6c-47d5-b3db-199b42a39f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 3ms/sample - loss: 0.8921 - acc: 0.7016\n",
      "[0.8921097209376674, 0.7016129]\n",
      "106/106 [==============================] - 0s 3ms/sample - loss: 0.5042 - acc: 0.8302\n",
      "[0.5042095791618779, 0.8301887]\n",
      "155/155 [==============================] - 1s 3ms/sample - loss: 0.7849 - acc: 0.7161\n",
      "[0.7848948702696831, 0.716129]\n",
      "115/115 [==============================] - 0s 3ms/sample - loss: 0.9274 - acc: 0.8174\n",
      "[0.9274278143177862, 0.8173913]\n"
     ]
    }
   ],
   "source": [
    "print (model.evaluate(x_valid[y_train_valid[ind_valid]==0], y_valid[y_train_valid[ind_valid]==0]))\n",
    "print (model.evaluate(x_valid[y_train_valid[ind_valid]==1], y_valid[y_train_valid[ind_valid]==1]))\n",
    "print (model.evaluate(x_valid[y_train_valid[ind_valid]==2], y_valid[y_train_valid[ind_valid]==2]))\n",
    "print (model.evaluate(x_valid[y_train_valid[ind_valid]==3], y_valid[y_train_valid[ind_valid]==3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB01dl8Xu0oG"
   },
   "source": [
    "### **Raw Data Isomap**\n",
    "Below we compute the Isomap of the hidden features extracted by the DCNN-dp-bn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "colab_type": "code",
    "id": "aD1nyq-laY2D",
    "outputId": "315f1397-3b5d-4d48-a825-9c922d86245b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Isomap embedding\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFZCAYAAABTxrzcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train1 = X_train_valid[ind_train,1,:].reshape((1615,1000))\n",
    "X = x_train1\n",
    "y = y_train_valid[ind_train]\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "               \n",
    "print(\"Computing Isomap embedding\")\n",
    "t0 = time()\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "plot_embedding(X_iso,\n",
    "               \"Isomap projection of the raw data (time %.2fs)\" %\n",
    "               (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LulcrCy5V8SQ"
   },
   "source": [
    "# **One subject based training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pe2SkqMBXL5L"
   },
   "source": [
    "### **Data Reloading** \n",
    "\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjoQhI1iWD4i"
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_test.npy\")\n",
    "y_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_test.npy\")\n",
    "person_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/y_train_valid.npy\")\n",
    "person_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_test.npy\")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yX5iE9bSXXvN"
   },
   "outputs": [],
   "source": [
    "score_sum_train = np.zeros((10,9))\n",
    "score_sum_val = np.zeros((10,9))\n",
    "score_sum_test = np.zeros((10,9))\n",
    "\n",
    "for i in range(10):\n",
    "    print ('iteration ' + str(i))\n",
    "    for chosen_class in range(9):\n",
    "        class_data = np.argwhere(person_train_valid==chosen_class)\n",
    "        class_data = class_data[:,0]\n",
    "        mask = np.ones(len(person_train_valid), dtype=bool)\n",
    "        mask[class_data] = False              # Set unwanted elements to False\n",
    "        remain_data = np.argwhere(mask== True)\n",
    "        remain_data = remain_data.reshape(remain_data.shape[0],)\n",
    "\n",
    "\n",
    "        class_data_2 = np.argwhere(person_test==chosen_class)\n",
    "        class_data_2 = class_data_2[:,0]\n",
    "        mask_2 = np.ones(len(person_test), dtype=bool)\n",
    "        mask_2[class_data_2] = False              # Set unwanted elements to False\n",
    "        remain_data_2 = np.argwhere(mask_2== True)\n",
    "        remain_data_2 = remain_data_2.reshape(remain_data_2.shape[0],)\n",
    "\n",
    "\n",
    "        X_train = X_train_valid[class_data]\n",
    "        y_train = y_train_valid[class_data]\n",
    "\n",
    "        X_val = X_test[class_data_2]\n",
    "        y_val = y_test[class_data_2]\n",
    "\n",
    "        # print (remain_data_2.shape)\n",
    "        # print (remain_data.shape)\n",
    "        # print (class_data.shape)\n",
    "        # print (class_data_2.shape)\n",
    "\n",
    "        X_t = np.concatenate((X_train_valid[remain_data], X_test[remain_data_2]), axis = 0)\n",
    "\n",
    "        y_t = np.concatenate((y_train_valid[remain_data], y_test[remain_data_2]), axis = 0)\n",
    "\n",
    "        print ('class:' + str(chosen_class))\n",
    "        #print (class_data.shape)\n",
    "        #print (X_train.shape)\n",
    "        #print (y_val.shape)\n",
    "        #print (X_t.shape)\n",
    "\n",
    "        #encode the labels\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "        y_val = tf.keras.utils.to_categorical(y_val, 4)\n",
    "        y_t = tf.keras.utils.to_categorical(y_t, 4)\n",
    "        #print (y_t.shape)\n",
    "\n",
    "        # reshape data to satisfy dimension inputs for CNN\n",
    "        X_train = np.swapaxes(X_train,2,1)\n",
    "        X_val = np.swapaxes(X_val,2,1)\n",
    "        X_t = np.swapaxes(X_t,2,1)\n",
    "\n",
    "        h = X_train.shape[1]\n",
    "        w = X_train.shape[2]\n",
    "\n",
    "        X_train = X_train.reshape(X_train.shape[0], h,1,w)\n",
    "        X_val = X_val.reshape(X_val.shape[0],h, 1,w)\n",
    "        X_t = X_t.reshape(X_t.shape[0],h,1,w)\n",
    "        #print (X_t.shape)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        #model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,25), padding='same', activation=''))\n",
    "        model.add(layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        #model.add(tf.keras.layers.Dense((50)))\n",
    "        #model.add(tf.keras.layers.Reshape((50,1)))\n",
    "        #model.add(tf.keras.layers.LSTM(20, dropout=0.5, recurrent_dropout=0.5, input_shape=(50,1), return_sequences=False))\n",
    "        model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "        #early stop\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0.0001,\n",
    "                                  patience=5,\n",
    "                                  verbose=0, mode='auto')\n",
    "\n",
    "        callback = [earlystop]\n",
    "\n",
    "        history = model.fit(X_train,\n",
    "             y_train,\n",
    "             batch_size= 64,\n",
    "             epochs=100, validation_data=(X_val, y_val), verbose=0, \n",
    "                            callbacks = callback)\n",
    "\n",
    "        score1 = model.evaluate(X_train, y_train, verbose = 0)\n",
    "        score_train = score1[1]\n",
    "        score2 = model.evaluate(X_val, y_val, verbose = 0 )\n",
    "        score_val = score2[1]\n",
    "        score3 = model.evaluate(X_t, y_t, verbose = 0)\n",
    "        score_test = score3[1]\n",
    "\n",
    "        #sum of scores\n",
    "        score_sum_train[i][chosen_class] = score_train\n",
    "        score_sum_val[i][chosen_class] = score_val\n",
    "        score_sum_test[i][chosen_class] = score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ojn798NlshS"
   },
   "source": [
    "# **Subject Identification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCrdxW4HPePL"
   },
   "source": [
    "### **Data Reloading**\n",
    "In the section below, we load the data. We relabel the data from 0-3 (instead of existing 769-773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaVIs9rOPdFG"
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"/path_name")\n",
    "y_test = np.load(\"path_name")\n",
    "person_train_valid = np.load(\"path_name")\n",
    "X_train_valid = np.load(\"path_name")\n",
    "y_train_valid = np.load(\"path_name")\n",
    "person_test = np.load(\"/content/gdrive/My Drive/ECE_239_AS/Project_final/project/person_test.npy\")\n",
    "\n",
    "y_train_valid[y_train_valid==769] = 0\n",
    "y_train_valid[y_train_valid==770] = 1\n",
    "y_train_valid[y_train_valid==771] = 2\n",
    "y_train_valid[y_train_valid==772] = 3\n",
    "\n",
    "y_test[y_test==769] = 0\n",
    "y_test[y_test==770] = 1\n",
    "y_test[y_test==771] = 2\n",
    "y_test[y_test==772] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjHJJCu3PyZP"
   },
   "source": [
    "### **Random Splitting and shaping of the Data**\n",
    "\n",
    "\n",
    "Splitting of the X_train_valid_data. We keep 500 data points out of 2115 for validation.\n",
    "\n",
    "We also reshape the data into a 4 dimensional tensor for inputting into CNN later.\n",
    "\n",
    "We also make the labels one hot encoded for inputting into cross entropy function for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Y1tS5Xyzlwhh",
    "outputId": "3436ee10-90af-4e30-809b-a2f9279b603d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1615, 25, 1000) y_train shape: (1615, 4)\n",
      "1615 train set\n",
      "500 validation set\n",
      "443 test set\n"
     ]
    }
   ],
   "source": [
    "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "ind_valid = np.random.choice(2115, 500, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "(x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "(person_train, person_valid) = person_train_valid[ind_train], person_train_valid[ind_valid]\n",
    "\n",
    "# Reshape input data from (25, 1000) to (28, 28, 1)\n",
    "w, h = 25, 1000\n",
    "x_train = x_train.reshape(x_train.shape[0], w, h)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], w, h)\n",
    "x_test = X_test.reshape(X_test.shape[0], w, h)\n",
    "\n",
    "# One-hot encode the labels\n",
    "\n",
    "#classify tasks\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 4)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 4)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "#classify people\n",
    "person_train = tf.keras.utils.to_categorical(person_train, 9)\n",
    "person_valid = tf.keras.utils.to_categorical(person_valid, 9)\n",
    "person_test_n = tf.keras.utils.to_categorical(person_test, 9)\n",
    "\n",
    "# Print training set shape\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Print the number of training, validation, and test datasets\n",
    "print(x_train.shape[0], 'train set')\n",
    "print(x_valid.shape[0], 'validation set')\n",
    "print(x_test.shape[0], 'test set')\n",
    "\n",
    "\n",
    "x_train = np.swapaxes(x_train,2,1)\n",
    "x_valid = np.swapaxes(x_valid,2,1)\n",
    "x_test = np.swapaxes(x_test,2,1)\n",
    "\n",
    "x_train = x_train.reshape(1615, h,1,w)\n",
    "x_valid = x_valid.reshape(500,h, 1,w)\n",
    "x_test = x_test.reshape(443,h,1,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OvudDM-QN8l"
   },
   "source": [
    "## Transfer Learning: Step 1. Train DCNN-dp-bn \n",
    "\n",
    "We will extract features from it to transfer knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-0XDPbLmPAC"
   },
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(1000,1,25))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides = (3,1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout))          \n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout))  \n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "    # model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    # model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3454
    },
    "colab_type": "code",
    "id": "i4p5IK6kmcix",
    "outputId": "936938b2-71f1-4f05-dd6f-594907511562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "1615/1615 [==============================] - 17s 11ms/sample - loss: 2.1974 - categorical_accuracy: 0.3022 - val_loss: 2.0345 - val_categorical_accuracy: 0.3540\n",
      "Epoch 2/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.7826 - categorical_accuracy: 0.4173 - val_loss: 1.2507 - val_categorical_accuracy: 0.5160\n",
      "Epoch 3/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.5279 - categorical_accuracy: 0.4793 - val_loss: 1.0553 - val_categorical_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.4125 - categorical_accuracy: 0.5220 - val_loss: 1.1339 - val_categorical_accuracy: 0.6040\n",
      "Epoch 5/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.3430 - categorical_accuracy: 0.5406 - val_loss: 1.0388 - val_categorical_accuracy: 0.6000\n",
      "Epoch 6/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.2753 - categorical_accuracy: 0.5684 - val_loss: 1.0403 - val_categorical_accuracy: 0.5920\n",
      "Epoch 7/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.1879 - categorical_accuracy: 0.5715 - val_loss: 0.9597 - val_categorical_accuracy: 0.5980\n",
      "Epoch 8/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.1274 - categorical_accuracy: 0.5907 - val_loss: 0.9563 - val_categorical_accuracy: 0.5960\n",
      "Epoch 9/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.0717 - categorical_accuracy: 0.6080 - val_loss: 0.9794 - val_categorical_accuracy: 0.6220\n",
      "Epoch 10/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.0329 - categorical_accuracy: 0.6341 - val_loss: 1.0039 - val_categorical_accuracy: 0.6160\n",
      "Epoch 11/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 1.0225 - categorical_accuracy: 0.6260 - val_loss: 0.9641 - val_categorical_accuracy: 0.6120\n",
      "Epoch 12/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.9751 - categorical_accuracy: 0.6421 - val_loss: 0.8920 - val_categorical_accuracy: 0.6440\n",
      "Epoch 13/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.9433 - categorical_accuracy: 0.6563 - val_loss: 0.9336 - val_categorical_accuracy: 0.6420\n",
      "Epoch 14/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.9435 - categorical_accuracy: 0.6464 - val_loss: 0.9062 - val_categorical_accuracy: 0.6400\n",
      "Epoch 15/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.9369 - categorical_accuracy: 0.6582 - val_loss: 0.8177 - val_categorical_accuracy: 0.6660\n",
      "Epoch 16/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8428 - categorical_accuracy: 0.6675 - val_loss: 0.8261 - val_categorical_accuracy: 0.6560\n",
      "Epoch 17/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8539 - categorical_accuracy: 0.6935 - val_loss: 0.7969 - val_categorical_accuracy: 0.6820\n",
      "Epoch 18/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8205 - categorical_accuracy: 0.6892 - val_loss: 0.8190 - val_categorical_accuracy: 0.6840\n",
      "Epoch 19/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8100 - categorical_accuracy: 0.7084 - val_loss: 0.8316 - val_categorical_accuracy: 0.6860\n",
      "Epoch 20/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8513 - categorical_accuracy: 0.6966 - val_loss: 0.7709 - val_categorical_accuracy: 0.6920\n",
      "Epoch 21/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.8008 - categorical_accuracy: 0.7053 - val_loss: 0.8700 - val_categorical_accuracy: 0.6720\n",
      "Epoch 22/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.7453 - categorical_accuracy: 0.7251 - val_loss: 0.7365 - val_categorical_accuracy: 0.7080\n",
      "Epoch 23/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.7465 - categorical_accuracy: 0.7269 - val_loss: 0.7605 - val_categorical_accuracy: 0.7020\n",
      "Epoch 24/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.7627 - categorical_accuracy: 0.7170 - val_loss: 0.7668 - val_categorical_accuracy: 0.7100\n",
      "Epoch 25/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6895 - categorical_accuracy: 0.7381 - val_loss: 0.8561 - val_categorical_accuracy: 0.6600\n",
      "Epoch 26/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6813 - categorical_accuracy: 0.7300 - val_loss: 0.8113 - val_categorical_accuracy: 0.6840\n",
      "Epoch 27/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6632 - categorical_accuracy: 0.7523 - val_loss: 0.7410 - val_categorical_accuracy: 0.7020\n",
      "Epoch 28/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6705 - categorical_accuracy: 0.7486 - val_loss: 0.8351 - val_categorical_accuracy: 0.6720\n",
      "Epoch 29/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6720 - categorical_accuracy: 0.7486 - val_loss: 0.7659 - val_categorical_accuracy: 0.6960\n",
      "Epoch 30/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.6349 - categorical_accuracy: 0.7666 - val_loss: 0.6977 - val_categorical_accuracy: 0.7180\n",
      "Epoch 31/100\n",
      "1615/1615 [==============================] - 11s 7ms/sample - loss: 0.6519 - categorical_accuracy: 0.7579 - val_loss: 0.7599 - val_categorical_accuracy: 0.7120\n",
      "Epoch 32/100\n",
      "1615/1615 [==============================] - 13s 8ms/sample - loss: 0.5874 - categorical_accuracy: 0.7715 - val_loss: 0.7828 - val_categorical_accuracy: 0.7040\n",
      "Epoch 33/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.6001 - categorical_accuracy: 0.7659 - val_loss: 0.7360 - val_categorical_accuracy: 0.7220\n",
      "Epoch 34/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.6143 - categorical_accuracy: 0.7641 - val_loss: 0.8265 - val_categorical_accuracy: 0.7020\n",
      "Epoch 35/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.5924 - categorical_accuracy: 0.7709 - val_loss: 0.7365 - val_categorical_accuracy: 0.7340\n",
      "Epoch 36/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.5680 - categorical_accuracy: 0.7789 - val_loss: 0.6982 - val_categorical_accuracy: 0.7380\n",
      "Epoch 37/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.5247 - categorical_accuracy: 0.7988 - val_loss: 0.7313 - val_categorical_accuracy: 0.7140\n",
      "Epoch 38/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.5583 - categorical_accuracy: 0.7870 - val_loss: 0.7578 - val_categorical_accuracy: 0.7020\n",
      "Epoch 39/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.5505 - categorical_accuracy: 0.7920 - val_loss: 0.7383 - val_categorical_accuracy: 0.7000\n",
      "Epoch 40/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.5523 - categorical_accuracy: 0.7858 - val_loss: 0.7406 - val_categorical_accuracy: 0.7220\n",
      "Epoch 41/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.5069 - categorical_accuracy: 0.8136 - val_loss: 0.7151 - val_categorical_accuracy: 0.7260\n",
      "Epoch 42/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.5273 - categorical_accuracy: 0.8012 - val_loss: 0.7406 - val_categorical_accuracy: 0.7300\n",
      "Epoch 43/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.4891 - categorical_accuracy: 0.8012 - val_loss: 0.7706 - val_categorical_accuracy: 0.7180\n",
      "Epoch 44/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.5034 - categorical_accuracy: 0.8050 - val_loss: 0.6970 - val_categorical_accuracy: 0.7640\n",
      "Epoch 45/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4914 - categorical_accuracy: 0.8111 - val_loss: 0.7510 - val_categorical_accuracy: 0.7440\n",
      "Epoch 46/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4657 - categorical_accuracy: 0.8130 - val_loss: 0.7407 - val_categorical_accuracy: 0.7240\n",
      "Epoch 47/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.5044 - categorical_accuracy: 0.8068 - val_loss: 0.7811 - val_categorical_accuracy: 0.7340\n",
      "Epoch 48/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4740 - categorical_accuracy: 0.8186 - val_loss: 0.7992 - val_categorical_accuracy: 0.7240\n",
      "Epoch 49/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4684 - categorical_accuracy: 0.8328 - val_loss: 0.7203 - val_categorical_accuracy: 0.7540\n",
      "Epoch 50/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.4231 - categorical_accuracy: 0.8272 - val_loss: 0.7734 - val_categorical_accuracy: 0.7300\n",
      "Epoch 51/100\n",
      "1615/1615 [==============================] - 16s 10ms/sample - loss: 0.4581 - categorical_accuracy: 0.8297 - val_loss: 0.8096 - val_categorical_accuracy: 0.7300\n",
      "Epoch 52/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4254 - categorical_accuracy: 0.8297 - val_loss: 0.7424 - val_categorical_accuracy: 0.7340\n",
      "Epoch 53/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4392 - categorical_accuracy: 0.8291 - val_loss: 0.7775 - val_categorical_accuracy: 0.7340\n",
      "Epoch 54/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4144 - categorical_accuracy: 0.8421 - val_loss: 0.7135 - val_categorical_accuracy: 0.7580\n",
      "Epoch 55/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4001 - categorical_accuracy: 0.8514 - val_loss: 0.7523 - val_categorical_accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.3968 - categorical_accuracy: 0.8495 - val_loss: 0.7602 - val_categorical_accuracy: 0.7320\n",
      "Epoch 57/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.4061 - categorical_accuracy: 0.8409 - val_loss: 0.7189 - val_categorical_accuracy: 0.7380\n",
      "Epoch 58/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.4064 - categorical_accuracy: 0.8322 - val_loss: 0.7471 - val_categorical_accuracy: 0.7460\n",
      "Epoch 59/100\n",
      "1615/1615 [==============================] - 15s 10ms/sample - loss: 0.3846 - categorical_accuracy: 0.8638 - val_loss: 0.8082 - val_categorical_accuracy: 0.7300\n",
      "Epoch 60/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.4095 - categorical_accuracy: 0.8520 - val_loss: 0.7738 - val_categorical_accuracy: 0.7380\n",
      "Epoch 61/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3820 - categorical_accuracy: 0.8514 - val_loss: 0.7514 - val_categorical_accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3941 - categorical_accuracy: 0.8483 - val_loss: 0.7611 - val_categorical_accuracy: 0.7580\n",
      "Epoch 63/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3460 - categorical_accuracy: 0.8619 - val_loss: 0.7594 - val_categorical_accuracy: 0.7580\n",
      "Epoch 64/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3745 - categorical_accuracy: 0.8539 - val_loss: 0.8399 - val_categorical_accuracy: 0.7340\n",
      "Epoch 65/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3622 - categorical_accuracy: 0.8588 - val_loss: 0.8150 - val_categorical_accuracy: 0.7340\n",
      "Epoch 66/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3810 - categorical_accuracy: 0.8619 - val_loss: 0.8426 - val_categorical_accuracy: 0.7260\n",
      "Epoch 67/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3512 - categorical_accuracy: 0.8768 - val_loss: 0.7588 - val_categorical_accuracy: 0.7540\n",
      "Epoch 68/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3270 - categorical_accuracy: 0.8786 - val_loss: 0.8540 - val_categorical_accuracy: 0.7180\n",
      "Epoch 69/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3257 - categorical_accuracy: 0.8706 - val_loss: 0.7817 - val_categorical_accuracy: 0.7340\n",
      "Epoch 70/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3319 - categorical_accuracy: 0.8842 - val_loss: 0.8321 - val_categorical_accuracy: 0.7300\n",
      "Epoch 71/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3348 - categorical_accuracy: 0.8768 - val_loss: 0.8495 - val_categorical_accuracy: 0.7300\n",
      "Epoch 72/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3478 - categorical_accuracy: 0.8687 - val_loss: 0.8624 - val_categorical_accuracy: 0.7280\n",
      "Epoch 73/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3526 - categorical_accuracy: 0.8613 - val_loss: 0.8573 - val_categorical_accuracy: 0.7260\n",
      "Epoch 74/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3202 - categorical_accuracy: 0.8762 - val_loss: 0.8012 - val_categorical_accuracy: 0.7340\n",
      "Epoch 75/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3176 - categorical_accuracy: 0.8811 - val_loss: 0.7816 - val_categorical_accuracy: 0.7540\n",
      "Epoch 76/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3130 - categorical_accuracy: 0.8885 - val_loss: 0.7780 - val_categorical_accuracy: 0.7380\n",
      "Epoch 77/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3140 - categorical_accuracy: 0.8774 - val_loss: 0.8247 - val_categorical_accuracy: 0.7380\n",
      "Epoch 78/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3022 - categorical_accuracy: 0.8836 - val_loss: 0.7586 - val_categorical_accuracy: 0.7640\n",
      "Epoch 79/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2839 - categorical_accuracy: 0.8885 - val_loss: 0.7675 - val_categorical_accuracy: 0.7420\n",
      "Epoch 80/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.3199 - categorical_accuracy: 0.8910 - val_loss: 0.8238 - val_categorical_accuracy: 0.7440\n",
      "Epoch 81/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.2688 - categorical_accuracy: 0.9022 - val_loss: 0.8563 - val_categorical_accuracy: 0.7440\n",
      "Epoch 82/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2555 - categorical_accuracy: 0.9108 - val_loss: 0.7895 - val_categorical_accuracy: 0.7380\n",
      "Epoch 83/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2789 - categorical_accuracy: 0.8991 - val_loss: 0.8213 - val_categorical_accuracy: 0.7300\n",
      "Epoch 84/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.2788 - categorical_accuracy: 0.8935 - val_loss: 0.7981 - val_categorical_accuracy: 0.7620\n",
      "Epoch 85/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2932 - categorical_accuracy: 0.8811 - val_loss: 0.8077 - val_categorical_accuracy: 0.7540\n",
      "Epoch 86/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.2752 - categorical_accuracy: 0.8978 - val_loss: 0.7941 - val_categorical_accuracy: 0.7600\n",
      "Epoch 87/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2611 - categorical_accuracy: 0.9003 - val_loss: 0.8008 - val_categorical_accuracy: 0.7520\n",
      "Epoch 88/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2412 - categorical_accuracy: 0.9152 - val_loss: 0.8789 - val_categorical_accuracy: 0.7500\n",
      "Epoch 89/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2484 - categorical_accuracy: 0.9015 - val_loss: 0.7833 - val_categorical_accuracy: 0.7760\n",
      "Epoch 90/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2332 - categorical_accuracy: 0.9195 - val_loss: 0.8032 - val_categorical_accuracy: 0.7620\n",
      "Epoch 91/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2338 - categorical_accuracy: 0.9090 - val_loss: 0.8560 - val_categorical_accuracy: 0.7520\n",
      "Epoch 92/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.2503 - categorical_accuracy: 0.9183 - val_loss: 0.8947 - val_categorical_accuracy: 0.7400\n",
      "Epoch 93/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2966 - categorical_accuracy: 0.8929 - val_loss: 0.9198 - val_categorical_accuracy: 0.7440\n",
      "Epoch 94/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2487 - categorical_accuracy: 0.8954 - val_loss: 0.8950 - val_categorical_accuracy: 0.7460\n",
      "Epoch 95/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2618 - categorical_accuracy: 0.8966 - val_loss: 0.8423 - val_categorical_accuracy: 0.7580\n",
      "Epoch 96/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2842 - categorical_accuracy: 0.9003 - val_loss: 0.8574 - val_categorical_accuracy: 0.7680\n",
      "Epoch 97/100\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 0.2431 - categorical_accuracy: 0.9146 - val_loss: 0.8004 - val_categorical_accuracy: 0.7540\n",
      "Epoch 98/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2349 - categorical_accuracy: 0.9077 - val_loss: 0.7984 - val_categorical_accuracy: 0.7580\n",
      "Epoch 99/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2221 - categorical_accuracy: 0.9176 - val_loss: 0.9402 - val_categorical_accuracy: 0.7540\n",
      "Epoch 100/100\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2376 - categorical_accuracy: 0.9090 - val_loss: 0.8469 - val_categorical_accuracy: 0.7560\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size= 64,\n",
    "         epochs=100, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vJgkWtenqXT"
   },
   "source": [
    "## Transfer Learning: Step 2. Extract Features From DCNN-dp-bn\n",
    "\n",
    "We extract the hidden layer values for each raw data point. We pass the data into the model and extract the output from the last convolutional layer after it is flattened. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sVP0BZlnqmb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "layer_name = 'flatten_10'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "x_train_pc = intermediate_layer_model.predict(x_train)\n",
    "x_valid_pc = intermediate_layer_model.predict(x_valid)\n",
    "x_test_pc = intermediate_layer_model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2qDvtc9n1Nr"
   },
   "outputs": [],
   "source": [
    "w = x_train_pc.shape[1]\n",
    "x_train_pc = x_train_pc.reshape(x_train_pc.shape[0], w, 1)\n",
    "x_valid_pc = x_valid_pc.reshape(x_valid_pc.shape[0], w,1)\n",
    "x_test_pc  = x_test_pc.reshape(x_test_pc.shape[0], w,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1WcXDV_vCWT"
   },
   "source": [
    "##Transfer Learning: Step 3 **Softmax on extracted features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "PEDWVHFin6Gx",
    "outputId": "23240597-ce8d-4042-e11c-ff7404a9d208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 200)               480200    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1809      \n",
      "=================================================================\n",
      "Total params: 482,009\n",
      "Trainable params: 482,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.Sequential()\n",
    "\n",
    "model1.add(tf.keras.layers.Flatten(input_shape=(w,1)))\n",
    "model1.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(9, activation='softmax'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "0QmWv5zcn_6I",
    "outputId": "f63cdc7f-9666-4dc0-b11c-bd502dcbfb66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 1s 651us/sample - loss: 1.4018 - acc: 0.4935 - val_loss: 0.7813 - val_acc: 0.7160\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 0s 213us/sample - loss: 0.2761 - acc: 0.9207 - val_loss: 0.5692 - val_acc: 0.8000\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 0s 213us/sample - loss: 0.0835 - acc: 0.9926 - val_loss: 0.5536 - val_acc: 0.8140\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 0s 212us/sample - loss: 0.0346 - acc: 1.0000 - val_loss: 0.5118 - val_acc: 0.8140\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 0s 214us/sample - loss: 0.0189 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8340\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 0s 202us/sample - loss: 0.0133 - acc: 1.0000 - val_loss: 0.5041 - val_acc: 0.8300\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 0s 211us/sample - loss: 0.0101 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8400\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 0s 211us/sample - loss: 0.0082 - acc: 1.0000 - val_loss: 0.4945 - val_acc: 0.8420\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 0s 200us/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4933 - val_acc: 0.8360\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 0s 203us/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.4968 - val_acc: 0.8420\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 0s 205us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.4943 - val_acc: 0.8420\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 0s 198us/sample - loss: 0.0044 - acc: 1.0000 - val_loss: 0.4936 - val_acc: 0.8400\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 0s 211us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.4946 - val_acc: 0.8420\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 0s 200us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.4952 - val_acc: 0.8400\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 0s 205us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8440\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 0s 210us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.4953 - val_acc: 0.8440\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 0s 204us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.8460\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 0s 212us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4966 - val_acc: 0.8500\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 0s 209us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4963 - val_acc: 0.8440\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 0s 207us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.4974 - val_acc: 0.8440\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 0s 198us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4964 - val_acc: 0.8440\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 0s 205us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4995 - val_acc: 0.8440\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 0s 195us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4994 - val_acc: 0.8460\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 0s 200us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5000 - val_acc: 0.8480\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 0s 216us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.5007 - val_acc: 0.8460\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 0s 210us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.5018 - val_acc: 0.8460\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 0s 208us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5021 - val_acc: 0.8480\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 0s 212us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 0.8460\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 0s 202us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5025 - val_acc: 0.8460\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 0s 211us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5025 - val_acc: 0.8460\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 0s 212us/sample - loss: 9.5147e-04 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.8460\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 0s 203us/sample - loss: 8.9931e-04 - acc: 1.0000 - val_loss: 0.5048 - val_acc: 0.8460\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 0s 203us/sample - loss: 8.5396e-04 - acc: 1.0000 - val_loss: 0.5048 - val_acc: 0.8440\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 0s 209us/sample - loss: 8.1039e-04 - acc: 1.0000 - val_loss: 0.5057 - val_acc: 0.8460\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 0s 217us/sample - loss: 7.6965e-04 - acc: 1.0000 - val_loss: 0.5047 - val_acc: 0.8460\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 0s 206us/sample - loss: 7.3270e-04 - acc: 1.0000 - val_loss: 0.5068 - val_acc: 0.8460\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 0s 205us/sample - loss: 6.9894e-04 - acc: 1.0000 - val_loss: 0.5064 - val_acc: 0.8480\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 0s 213us/sample - loss: 6.6680e-04 - acc: 1.0000 - val_loss: 0.5078 - val_acc: 0.8460\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 0s 213us/sample - loss: 6.3639e-04 - acc: 1.0000 - val_loss: 0.5079 - val_acc: 0.8480\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 0s 210us/sample - loss: 6.0940e-04 - acc: 1.0000 - val_loss: 0.5082 - val_acc: 0.8480\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 0s 216us/sample - loss: 5.8252e-04 - acc: 1.0000 - val_loss: 0.5098 - val_acc: 0.8480\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 0s 203us/sample - loss: 5.5664e-04 - acc: 1.0000 - val_loss: 0.5097 - val_acc: 0.8480\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 0s 204us/sample - loss: 5.3404e-04 - acc: 1.0000 - val_loss: 0.5111 - val_acc: 0.8500\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 0s 203us/sample - loss: 5.1224e-04 - acc: 1.0000 - val_loss: 0.5116 - val_acc: 0.8500\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 0s 202us/sample - loss: 4.9155e-04 - acc: 1.0000 - val_loss: 0.5127 - val_acc: 0.8500\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 0s 199us/sample - loss: 4.7216e-04 - acc: 1.0000 - val_loss: 0.5119 - val_acc: 0.8500\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 0s 211us/sample - loss: 4.5373e-04 - acc: 1.0000 - val_loss: 0.5129 - val_acc: 0.8500\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 0s 202us/sample - loss: 4.3670e-04 - acc: 1.0000 - val_loss: 0.5141 - val_acc: 0.8500\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 0s 208us/sample - loss: 4.2043e-04 - acc: 1.0000 - val_loss: 0.5140 - val_acc: 0.8480\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 0s 206us/sample - loss: 4.0557e-04 - acc: 1.0000 - val_loss: 0.5142 - val_acc: 0.8500\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(x_train_pc,\n",
    "         person_train,\n",
    "         batch_size= 64,\n",
    "         epochs=50, validation_data=(x_valid_pc, person_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XRN-RU0GuSda",
    "outputId": "aa1777a2-ddfb-4781-8c92-5d78e00b220b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 85us/sample - loss: 0.9587 - acc: 0.6820\n",
      "0.682\n"
     ]
    }
   ],
   "source": [
    "score = model1.evaluate(x_valid_pc, person_valid)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcEnzZuCoIBB"
   },
   "source": [
    "## **Direct Learning for Subject Identification**\n",
    "\n",
    "Just use raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "9EPOSz-AoEKJ",
    "outputId": "1b8d1964-4ebb-43b1-e57a-20a99a89e47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 1000, 1, 25)       5650      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 334, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 334, 1, 25)        100       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 334, 1, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 334, 1, 50)        11300     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 112, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 112, 1, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 112, 1, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 112, 1, 100)       50100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 38, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 38, 1, 100)        400       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 38, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 38, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 12, 1, 200)        800       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 12, 1, 200)        0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 9)                 21609     \n",
      "=================================================================\n",
      "Total params: 290,359\n",
      "Trainable params: 289,609\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "model2.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(3,3), padding='same', activation='elu', input_shape=(1000,1,25)))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model2.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(3,3), padding='same', activation='elu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model2.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Dropout(0.5))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "model2.add(tf.keras.layers.BatchNormalization())\n",
    "model2.add(tf.keras.layers.Dropout(dropout))  \n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(9, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "ByPcEqDGoG6K",
    "outputId": "23fb90a8-adfe-42b4-c5c3-9fa643883f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1615 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1615/1615 [==============================] - 15s 9ms/sample - loss: 2.3203 - acc: 0.3251 - val_loss: 6.3930 - val_acc: 0.2620\n",
      "Epoch 2/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 1.4500 - acc: 0.5300 - val_loss: 3.6863 - val_acc: 0.2580\n",
      "Epoch 3/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.9997 - acc: 0.6539 - val_loss: 1.9585 - val_acc: 0.5120\n",
      "Epoch 4/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.7080 - acc: 0.7430 - val_loss: 1.6862 - val_acc: 0.5500\n",
      "Epoch 5/50\n",
      "1615/1615 [==============================] - 14s 8ms/sample - loss: 0.5032 - acc: 0.8272 - val_loss: 1.7168 - val_acc: 0.5200\n",
      "Epoch 6/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3825 - acc: 0.8650 - val_loss: 1.0803 - val_acc: 0.6540\n",
      "Epoch 7/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.3016 - acc: 0.8960 - val_loss: 1.0541 - val_acc: 0.6620\n",
      "Epoch 8/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2386 - acc: 0.9207 - val_loss: 1.0493 - val_acc: 0.6720\n",
      "Epoch 9/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.2024 - acc: 0.9325 - val_loss: 1.1601 - val_acc: 0.6640\n",
      "Epoch 10/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.1558 - acc: 0.9424 - val_loss: 1.7646 - val_acc: 0.6100\n",
      "Epoch 11/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.1105 - acc: 0.9641 - val_loss: 1.6438 - val_acc: 0.6620\n",
      "Epoch 12/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.1326 - acc: 0.9585 - val_loss: 2.2650 - val_acc: 0.6020\n",
      "Epoch 13/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0855 - acc: 0.9690 - val_loss: 1.5018 - val_acc: 0.6860\n",
      "Epoch 14/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.1014 - acc: 0.9690 - val_loss: 1.7164 - val_acc: 0.6580\n",
      "Epoch 15/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.1185 - acc: 0.9598 - val_loss: 1.1380 - val_acc: 0.7200\n",
      "Epoch 16/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0670 - acc: 0.9759 - val_loss: 1.2240 - val_acc: 0.7280\n",
      "Epoch 17/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0667 - acc: 0.9777 - val_loss: 1.3905 - val_acc: 0.7140\n",
      "Epoch 18/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0499 - acc: 0.9820 - val_loss: 1.0694 - val_acc: 0.7640\n",
      "Epoch 19/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0741 - acc: 0.9752 - val_loss: 1.3367 - val_acc: 0.7180\n",
      "Epoch 20/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0442 - acc: 0.9870 - val_loss: 1.2404 - val_acc: 0.7360\n",
      "Epoch 21/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0468 - acc: 0.9802 - val_loss: 1.6301 - val_acc: 0.6840\n",
      "Epoch 22/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0604 - acc: 0.9814 - val_loss: 1.1615 - val_acc: 0.7480\n",
      "Epoch 23/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0451 - acc: 0.9858 - val_loss: 1.1039 - val_acc: 0.7600\n",
      "Epoch 24/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0629 - acc: 0.9789 - val_loss: 1.2736 - val_acc: 0.7540\n",
      "Epoch 25/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0424 - acc: 0.9839 - val_loss: 1.3565 - val_acc: 0.7400\n",
      "Epoch 26/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0378 - acc: 0.9907 - val_loss: 0.8872 - val_acc: 0.8020\n",
      "Epoch 27/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0215 - acc: 0.9932 - val_loss: 0.9403 - val_acc: 0.7700\n",
      "Epoch 28/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0310 - acc: 0.9889 - val_loss: 1.0965 - val_acc: 0.7480\n",
      "Epoch 29/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0676 - acc: 0.9734 - val_loss: 1.0803 - val_acc: 0.7680\n",
      "Epoch 30/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0411 - acc: 0.9864 - val_loss: 1.6851 - val_acc: 0.6920\n",
      "Epoch 31/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0606 - acc: 0.9802 - val_loss: 1.5712 - val_acc: 0.7200\n",
      "Epoch 32/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0759 - acc: 0.9752 - val_loss: 1.3334 - val_acc: 0.7380\n",
      "Epoch 33/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0448 - acc: 0.9833 - val_loss: 1.3416 - val_acc: 0.7400\n",
      "Epoch 34/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0205 - acc: 0.9920 - val_loss: 0.7424 - val_acc: 0.8240\n",
      "Epoch 35/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0208 - acc: 0.9926 - val_loss: 0.8849 - val_acc: 0.8000\n",
      "Epoch 36/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0269 - acc: 0.9920 - val_loss: 1.2888 - val_acc: 0.7280\n",
      "Epoch 37/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0161 - acc: 0.9926 - val_loss: 1.3079 - val_acc: 0.7340\n",
      "Epoch 38/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0186 - acc: 0.9926 - val_loss: 1.2371 - val_acc: 0.7600\n",
      "Epoch 39/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0147 - acc: 0.9963 - val_loss: 1.3050 - val_acc: 0.7340\n",
      "Epoch 40/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0079 - acc: 0.9988 - val_loss: 1.1489 - val_acc: 0.7700\n",
      "Epoch 41/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0137 - acc: 0.9944 - val_loss: 1.1050 - val_acc: 0.7660\n",
      "Epoch 42/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0147 - acc: 0.9944 - val_loss: 0.8455 - val_acc: 0.8020\n",
      "Epoch 43/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0076 - acc: 0.9969 - val_loss: 0.6920 - val_acc: 0.8360\n",
      "Epoch 44/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0152 - acc: 0.9950 - val_loss: 0.9011 - val_acc: 0.8020\n",
      "Epoch 45/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0160 - acc: 0.9938 - val_loss: 0.6750 - val_acc: 0.8520\n",
      "Epoch 46/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0202 - acc: 0.9926 - val_loss: 1.2912 - val_acc: 0.7500\n",
      "Epoch 47/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 1.2400 - val_acc: 0.7620\n",
      "Epoch 48/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0153 - acc: 0.9938 - val_loss: 0.9521 - val_acc: 0.8000\n",
      "Epoch 49/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0122 - acc: 0.9969 - val_loss: 1.2261 - val_acc: 0.7760\n",
      "Epoch 50/50\n",
      "1615/1615 [==============================] - 14s 9ms/sample - loss: 0.0145 - acc: 0.9932 - val_loss: 1.2091 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(x_train,\n",
    "         person_train,\n",
    "         batch_size= 64,\n",
    "         epochs=50, validation_data=(x_valid, person_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCB6MkfbVHVg"
   },
   "source": [
    "## **Transfer Learning vs Direct Learning**\n",
    "\n",
    "In the section below, we compare the performance accuracy of the transfer learning and direct learning approach for different amounts of data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxOQBYv4VGJr"
   },
   "outputs": [],
   "source": [
    "Niter = 5\n",
    "Eval1 = np.zeros(Niter)\n",
    "Eval2 = np.zeros(Niter)\n",
    "Frac = np.linspace(0,1,Niter)\n",
    "\n",
    "for iter in range(Niter):\n",
    "  subset_size = x_train_pc.shape[0]*(iter+1)//Niter \n",
    "  index = np.random.choice(x_train_pc.shape[0], subset_size, replace=False)\n",
    "  for m in range(5):\n",
    "    model1 = tf.keras.Sequential()\n",
    "\n",
    "    model1.add(tf.keras.layers.Flatten(input_shape=(w,1)))\n",
    "  #   model1.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "    model1.add(tf.keras.layers.Dense(9, activation='softmax'))\n",
    "\n",
    "#     model1.summary()\n",
    "\n",
    "\n",
    "    model1.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    history1 = model1.fit(x_train_pc[index],\n",
    "           person_train[index],\n",
    "           batch_size= 64,\n",
    "           epochs=50, validation_data=(x_valid_pc, person_valid), verbose=False) \n",
    "    model2 = tf.keras.Sequential()\n",
    "\n",
    "    model2.add(tf.keras.layers.Conv2D(filters=25, kernel_size=(3,3), padding='same', activation='elu', input_shape=(1000,1,25)))\n",
    "    model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "    model2.add(tf.keras.layers.BatchNormalization())\n",
    "    model2.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    #model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(10,25), padding='same', activation=''))\n",
    "    model2.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(3,3), padding='same', activation='elu'))\n",
    "    model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "    model2.add(tf.keras.layers.BatchNormalization())\n",
    "    model2.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model2.add(tf.keras.layers.Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same'))\n",
    "    model2.add(tf.keras.layers.BatchNormalization())\n",
    "    model2.add(tf.keras.layers.Dropout(0.5))\n",
    "    model2.add(tf.keras.layers.Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    model2.add(tf.keras.layers.MaxPooling2D(pool_size=(3,1), strides=(3,1)))\n",
    "    model2.add(tf.keras.layers.BatchNormalization())\n",
    "    model2.add(tf.keras.layers.Dropout(dropout))  \n",
    "    model2.add(tf.keras.layers.Flatten())\n",
    "    model2.add(tf.keras.layers.Dense(9, activation='softmax'))\n",
    "\n",
    "#     model2.summary()\n",
    "\n",
    "    model2.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    history2 = model2.fit(x_train[index],\n",
    "           person_train[index],\n",
    "           batch_size= 64,\n",
    "           epochs=50, validation_data=(x_valid, person_valid), verbose=False)\n",
    "\n",
    "\n",
    "    score1 = model1.evaluate(x_valid_pc, person_valid)\n",
    "    Eval1[iter] +=      score1[1]\n",
    "\n",
    "    score2 = model2.evaluate(x_valid, person_valid)\n",
    "    Eval2[iter] +=      score2[1]  \n",
    "\n",
    "Eval1 = Eval1/5\n",
    "Eval2 = Eval2/5\n",
    "\n",
    "Frac = np.linspace(0,1,Niter)\n",
    "from matplotlib import pyplot  \n",
    "pyplot.ylabel('Accuracy')\n",
    "pyplot.xlabel('Fraction of the data used')\n",
    "pyplot.plot(Frac,Eval1)   \n",
    "\n",
    "pyplot.plot(Frac,Eval2)  \n",
    "pyplot.legend(('Transfer based','Raw data based'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bg15Z6QyY9c4",
    "stvm7jQNgmux",
    "QiCyNg7HZtBW",
    "p865YZVcZQM4",
    "og_cjRhKcs-l",
    "gxTZeZUCXFYr",
    "s-UzeaGmYHSt",
    "v8v5ghUPMt1d",
    "eNg4czpSMjAd",
    "-NUZbxNINRu7",
    "06-F-0NgwTXf",
    "2qtnOV5igCH6",
    "0gN__CR2gapc",
    "2l-xw7DoldgK",
    "6VbOKqtbn_Fw",
    "0oKfIKhdlFYK",
    "W5446miLpWOC",
    "FCd8o5tBq1oJ",
    "ro3Wx0mrrJ3o",
    "UdOk-1PVNH7G",
    "_40f91D6svHN",
    "SaqsI1Ew4Y0p",
    "3dAAVFwI7Szx",
    "0sCI1ZbcbPnJ",
    "QOzSi8QPCaLx",
    "C-yWKBGUCzG2",
    "4GIxqwTuDAIT",
    "5uiE0VZhaeNf",
    "pfgxvr11G5Ml",
    "9Gs3cNumJqTH",
    "n3StzGLOuSiQ",
    "fAycCQYAO4aC",
    "mku7krXguaq9",
    "fB01dl8Xu0oG",
    "LulcrCy5V8SQ",
    "7ojn798NlshS",
    "JCrdxW4HPePL",
    "gjHJJCu3PyZP",
    "7OvudDM-QN8l",
    "7vJgkWtenqXT",
    "u1WcXDV_vCWT",
    "pcEnzZuCoIBB",
    "RCB6MkfbVHVg"
   ],
   "name": "EEG_loading_March_17_jmtd.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
